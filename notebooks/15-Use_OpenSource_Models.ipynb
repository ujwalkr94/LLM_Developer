{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ujwalkr94/LLM_Developer/blob/main/notebooks/15-Use_OpenSource_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEBIPbOzcUYf"
      },
      "source": [
        "(Updated Notebook)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zE1h0uQV7uT"
      },
      "source": [
        "# Install Packages and Setup Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "QPJzr-I9XQ7l",
        "outputId": "296a67fc-7800-4f83-eb47-48acedef66b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m599.1/599.1 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m602.1/602.1 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m100.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.9/194.9 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.7/96.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.9/265.9 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.4/303.4 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.9/42.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m186.1/186.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.5/259.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m94.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.4/207.4 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.4/115.4 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/45.1 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for html2text (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for spider-client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.8/431.8 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q llama-index==0.12.2 openai==1.70.0 llama-index-llms-gemini==0.4.1 tiktoken==0.8.0 chromadb==0.6.0\n",
        "!pip install -q llama-index-vector-stores-chroma==0.4.1 llama-index-finetuning==0.3.1 llama-index-llms-together==0.3.1\n",
        "!pip install -q llama-index-embeddings-huggingface==0.4.0 llama-index-readers-web==0.3.1 html2text==2024.2.26\n",
        "!pip install -q sentence-transformers==3.3.1 pydantic==2.10.4 pandas==2.2.2 # kaleido==0.2.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gtybmfl8TSBU"
      },
      "outputs": [],
      "source": [
        "# !pip install -q openai==1.70.0 chromadb==0.6.0 google-generativeai==0.8.4 langchain==0.3.25 langchain-chroma==0.2.3\n",
        "\n",
        "# langchain-openai==0.3.16 langchain_google_genai==2.0.9 llama-index-llms-gemini==0.4.1 llama-index==0.12.21\n",
        "\n",
        "# llama-index-vector-stores-chroma==0.4.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riuXwpSPcvWC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Set environment variables for the API keys\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"<YOUR_API_KEY>\"\n",
        "os.environ[\"TOGETHER_AI_API_TOKEN\"] = \"<YOUR_API_KEY>\"\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"<YOUR_API_KEY>\"\n",
        "\n",
        "# from google.colab import userdata\n",
        "# os.environ[\"OPENAI_API_KEY\"] = userdata.get('openai_api_key')\n",
        "# os.environ[\"GOOGLE_API_KEY\"] = userdata.get('Google_api_key')\n",
        "# os.environ[\"TOGETHER_AI_API_TOKEN\"] = userdata.get('together_api_key')\n",
        "\n",
        "\n",
        "# Allows running asyncio in environments with an existing event loop, like Jupyter notebooks.\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BwVuJXlzHVL"
      },
      "source": [
        "# Create a vector store and ingest articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQP87lHczHKc"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "\n",
        "# create vector store\n",
        "vector_store_name = \"mini-llama-articles\"\n",
        "chroma_client = chromadb.PersistentClient(path=vector_store_name)\n",
        "chroma_collection = chroma_client.get_or_create_collection(vector_store_name)\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZwf6pv7WFmD"
      },
      "source": [
        "The dataset includes several articles from the TowardsAI blog, which provide an in-depth explanation of the LLaMA2 model. Read the dataset as a long string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wl_pbPvMlv1h",
        "outputId": "d8ae544d-0d0c-407a-84b0-19818ed4d7e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  169k  100  169k    0     0   426k      0 --:--:-- --:--:-- --:--:--  426k\n"
          ]
        }
      ],
      "source": [
        "!curl -o ./mini-llama-articles.csv https://raw.githubusercontent.com/AlaFalaki/tutorial_notebooks/main/data/mini-llama-articles.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWBLtDbUWJfA"
      },
      "source": [
        "## Read articles from file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Q9sxuW0g3Gd"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "rows = []\n",
        "\n",
        "# Load the file as a JSON\n",
        "with open(\"./mini-llama-articles.csv\", mode=\"r\", encoding=\"utf-8\") as file:\n",
        "  csv_reader = csv.reader(file)\n",
        "\n",
        "  for idx, row in enumerate( csv_reader ):\n",
        "    if idx == 0: continue; # Skip header row\n",
        "    rows.append(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S17g2RYOjmf2"
      },
      "source": [
        "## Ingest documents into vector store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433,
          "referenced_widgets": [
            "133ad5fb61d747a4bf0be2d6f5b2dec0",
            "9fd34bf80a874011b06aac44f174a1d4",
            "2d505f1aa0a6456797efb33f6e0b84fc",
            "c603f36d174b4b2b91e36f66a210d3cc",
            "82ea9bf2edc0477288140a00f066d799",
            "0fa4ae272adc44f99ab956ef254fccea",
            "d545e6396a794f84969873a435b6dd93",
            "f6184585aff3446d869ce00abc287a02",
            "90bf498f5b6d4a0998b08bfb9772f297",
            "7bb37216db3443849f42f6110e47452a",
            "a25f57c63364486787fad671da3882b0",
            "1a05e192534241159418df92d6ae993e",
            "dc627a2c06f04a408f56f8142d106edc",
            "9e66c4b129a242e6a37e6199cfc1d2c3",
            "6653e83f40aa4303a3db554664b4e080",
            "7e8ae211eb184e5ca2c3a35cb7718a6c",
            "6bbb2240134f4be295860703f1706494",
            "afd83f2b2fe94ecc958ede3ebc635110",
            "9ab2efdad85a4ed78c04b39d84d6f05f",
            "80281772ff35450186b2a2c2ff1f810f",
            "4d835b1094474d4d8259c23c578dd706",
            "e81adde4fd784526a9185a5e829f451b",
            "f0a6008012574a37808317130e10fbb3",
            "635c583354c1494c951b2fae4dcb62a0",
            "0180e8f48f204c5fb48f5a5a39a3eba0",
            "8464a8d899f6452f87c0e32fda57b9c2",
            "b67bd499030942189ce37bdce2c9b924",
            "632b25262d4d4e6598917a2ef0fc6ad5",
            "97d34ef792d14f6c9c09fb2d7320ef07",
            "64f7e4b091c94e348196634f54fd18b0",
            "29c8f6f819fc4b3284d2552ab8b471c4",
            "521a40f2c44f444fa27071da98a0168e",
            "6d8dd3cf63964bcda6328fb51697c315",
            "0417719518ab45529a5b38b5cf2f11c7",
            "275005442f2549609b71aaf94a3c1c13",
            "8d760a6eaa2c42458d08d40f8027c3a6",
            "2147e1f03c8b423487cec7d83d639390",
            "48a2a9af6bb44ff0861874f1030ba61e",
            "de22ad501da646efb35b5b6a5a810503",
            "d687ecef300940afafcdcf77d1e4c42a",
            "6f55c4c3e2644b58811f699dc58651eb",
            "39f47a964adb445fb0645df381a87663",
            "43b954a50a284d3491d4660b8e6dfe5f",
            "8d4a900e607b4aa6bf27ceff0dd9f032",
            "27929b2f2b7f41e9ad406ddf70e87606",
            "196e115117c64f358e4668f8068dd23a",
            "4f3302ef832a4119805b5d1b41cd1065",
            "8f14ee438ba14a5588ab9a14f499e1aa",
            "fd847209aba946f1853caf04732030fd",
            "63c9681c096a4e92b65631656fda0553",
            "3a47745c479f4b6395cc9bf6e3c6cc3a",
            "6494a898eee9483ead4b424432f5094f",
            "dda0173f07a64da991ae3789f7b3fda6",
            "4e9a9605a1914b0a9917fdc70facb32b",
            "d1c10d5bd70f4b748a6f98686085d397",
            "257b4b674113405e9800c660e720d364",
            "a26cab89207848c79cee9df1cf203dc8",
            "b4779a7524ad4a78a4ce5a28a0ffd3bb",
            "99d6b47a96954e1e9a76e04d79a3d57f",
            "389fbd0c04f1435aa39da2318d3accdd",
            "6879ef91ec9c4d90903707edbac295cb",
            "3c5b7d4046d149aeafc6fd021f274c94",
            "8c94f8fca9094b83bc65fe4bf1342fd2",
            "329c4f83eb3a46589666a127a02cb000",
            "fe446daf03d84a7892a0ef8c52747f4c",
            "2e8783f13d164285980a723fbdad2931",
            "867d75d7f1c043d0b7db7b5ab2d754da",
            "b354c8930d194d22b44531d2f09ee280",
            "73ba979875af492ebb252755cdefe763",
            "c2f025b6c2b846b0b8c54ac1fe97c024",
            "46ebfecf0d5a4a9687ae7f75c60a85cb",
            "ed2654c7514c482e8156507d95ad9fba",
            "6d0cf35a2f8f498ca8d06f353cfa7649",
            "1a2841afdf6548efb7e4f1d041a68c4d",
            "70b4fde2ae844241854872a575b00898",
            "cda3c74224a84cf8ab4c511a2cf8440d",
            "de894b6b2053479d8ce78b3d41ee4c49",
            "ea4d6626b660409ca8e61533fdff4b20",
            "e8fc6570b5a244b9b48011ab00f3c00e",
            "028c97ebc4b249959ee1fc63bc01148e",
            "27c906c7f1aa47b88e7bc2ef6120bb1f",
            "1dd05049afa94c41b62beedc386cebf5",
            "251e3ba88fee43479db218069d61acf4",
            "94041a1b64094ccb947b7f9aa5cfb2af",
            "a4eaa3ed62ce434091314709a6b0446e",
            "053394bf932347c7b09f8cc18ca24590",
            "0b7a70132b2e4287a3f5b52adb1b7091",
            "4887cb1daefc439da26ddc7e195e0e49",
            "3c8304c2027440b4bf8d04cf623258d1",
            "7ad1484bf72e4b90b3c35f711953b29a",
            "765c994e90af431a829a4d11bbf6adb6",
            "85baf3388fad47f58e4f78216144f62e",
            "9cb7f086173f42139c882df08a0ca0c5",
            "2662c5b8e2b3420899a5f4791279ebb0",
            "252d85064c334a11ac97d9e48d6375b4",
            "2732275154184b88b11339a4b98ce9f7",
            "41891a190f5c42fb9a7f56a1db855a8d",
            "c146dfe403a0413cb676a9d74731cca8",
            "ccacd1d202344724b7789a4829a5ac97",
            "490048b9bf5d4956a29e92c691b9708f",
            "c21e95047c7c4dac89c8e18cee9e5ae0",
            "6adc9dc7e0464df984c753b2d5cedf67",
            "cdee4fadef8247d5a6ea4939efe23dc0",
            "ee5d5523226a41cfb6b140958b79e2e3",
            "bcd398ca16b14bb58a361ea98224c87b",
            "873c65f97e6e4b18938886244c78984e",
            "c6dc78227d54499b9a34480ad4662ab7",
            "f6b1f9988d464d8dacae7ab081d38544",
            "87e7d9a8d78649dd90fe1c4b8f953362",
            "d4867fb96d51436ea6b60eb9ba9f998b",
            "7ccba4ad436c4a5c98200f6eb5e26493",
            "77fcb08431be4e6c96e0eccbfca862db",
            "7f6fcc4e85ca450898a59f9b645b495d",
            "af4ac42fbb5444b39a335de8d98eaca7",
            "08df262d4f0f4401a910f4cac30a7c79",
            "aa273f1ab950478f8fb04a0b13a5411e",
            "5ae30a7b37a845c8bad6d6c095fced8c",
            "cae4bb3b630247bba7b31b3e699f3971",
            "9da4cac5718442d08382112936f991f6",
            "93e074a585654a3b86666ae3e69d46ee",
            "ea22e19f47b64ba68fbfc93ad4556d05",
            "97c3ea5253a74bd0ad40034121bc6018",
            "f6db8e2d61ed433986f218df0b9c9b2c",
            "6f0a16ca982e447bbf6616ad1c41fb82",
            "99040389818648ea8eccc9c14e33d39b",
            "b236cf5eb7ad41d5b62b4bdfa3d0073b",
            "64285b45c8294adf9a0166fa2adfa6c4",
            "007a13ab10e64016bd51bbb7996bb041",
            "051ba0dcf0f64c2886dd749f4217df99",
            "0d4335044b564f778d8694f625f7c8b1",
            "eaf1cac26406465aa8829113ee99be60",
            "655049c4e03a41aa849b4c03de4ae1d0",
            "c485811c973d432db985d320b5d24e4f",
            "4f8ff09a590d4039856667a58f514e35",
            "5ac7dde9fc384b5eab7b4c03d3362839",
            "0bcdb3b1690841f0bd8befff9f7e95a5",
            "d3c5d654bf25421689bc9ff28d9ca914",
            "43d82a32f7fe47b084207485e4e91be0",
            "c94e7cd713eb44fea5c8655b2f16f2b4",
            "b55a72476bf0415790c7e6d9633cec0e",
            "78dfdd7e1a524db0b956cb042f34959e",
            "096906a24cf1448594d7c62beb1a645b",
            "a5c74e36855b44c2be8e7cabae961f79"
          ]
        },
        "id": "YizvmXPejkJE",
        "outputId": "331cc091-b68b-4a89-f389-d62bc8cc6424"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "133ad5fb61d747a4bf0be2d6f5b2dec0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1a05e192534241159418df92d6ae993e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0a6008012574a37808317130e10fbb3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/94.8k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0417719518ab45529a5b38b5cf2f11c7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "27929b2f2b7f41e9ad406ddf70e87606",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "257b4b674113405e9800c660e720d364",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "867d75d7f1c043d0b7db7b5ab2d754da",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ea4d6626b660409ca8e61533fdff4b20",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c8304c2027440b4bf8d04cf623258d1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "490048b9bf5d4956a29e92c691b9708f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7ccba4ad436c4a5c98200f6eb5e26493",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "97c3ea5253a74bd0ad40034121bc6018",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Parsing nodes:   0%|          | 0/14 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c485811c973d432db985d320b5d24e4f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating embeddings:   0%|          | 0/108 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from llama_index.core import Document\n",
        "from llama_index.core.text_splitter import TokenTextSplitter\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core.ingestion import IngestionPipeline\n",
        "\n",
        "# Convert the chunks to Document objects so the LlamaIndex framework can process them.\n",
        "documents = [Document(text=row[1], metadata={\"title\": row[0], \"url\": row[2], \"source_name\": row[3]}) for row in rows]\n",
        "\n",
        "# Define the splitter object that split the text into segments with 512 tokens,\n",
        "# with a 128 overlap between the segments.\n",
        "text_splitter = TokenTextSplitter(\n",
        "    separator=\" \", chunk_size=512, chunk_overlap=128\n",
        ")\n",
        "\n",
        "# Create the pipeline to apply the transformation on each chunk,\n",
        "# and store the transformed text in the chroma vector store.\n",
        "pipeline = IngestionPipeline(\n",
        "    transformations=[\n",
        "        text_splitter,\n",
        "        HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\") # Or, OpenAIEmbedding()\n",
        "    ],\n",
        "    vector_store=vector_store\n",
        ")\n",
        "\n",
        "nodes = pipeline.run(documents=documents, show_progress=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWaT6rL7ksp8"
      },
      "source": [
        "# Load vector store and create query engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369,
          "referenced_widgets": [
            "3cb088fe31a2479093e70161d0f5ae1c",
            "699da494f3ca4805855bc53b2e05c6d8",
            "a36b96dcc74a4662a03ac0a4842894e6",
            "eb8e75d1237045219e100181b131aa47",
            "0b3980f798ab4d6294a454c871199f21",
            "5a02c3d5bb9847058a72ac6c47dd480a",
            "01e7563438f24303b2fad95369d2e32e",
            "ec4bb8dc973e4987a255d195408a3e07",
            "835f510682d84c11975d3e243b48bf75",
            "170704b8ab88469cbe19e95cfc9d9df9",
            "f275ceaa231a4ff08ea2ccd514331af3",
            "1c4071860fe045dbae85b93f50334871",
            "f947ebcef598490e83a23c370ef25343",
            "88228aed0ad044edb0040ec07e98567b",
            "5b4a1dee83264e8cb206c36ea70d60d5",
            "3151819239404032bbbbce1628fb7ff9",
            "1599a904013245d3b3750e6785db4a83",
            "ed2d9f442f9c4bba9c4457331508290b",
            "6e4fb449a7c2412ea2ed71ce68276531",
            "de7f47c0ff2a4db09b19abeaaa61ac66",
            "f4e68d4140a44e698ad8eca34b0e96a0",
            "d14953d65c4a446293abd7d1921d7848",
            "b7a72b115b8a4d4cb7843db2488563be",
            "06c914aba41545a38694b5f480938ebc",
            "84d45ebb700843299fa6e2b4f2be31de",
            "20625bca02bf4ee186a140b64ae0c1ee",
            "eacffc5953a14dfd9fe3ac11da0898d0",
            "4f108967f6e14e89bb2bb38daf0b8101",
            "644342620be145de943a523d30eec6cc",
            "65905c72387447feb73cab0643dd8afe",
            "2e6740dd82fd4425b020ada2cf5a84b9",
            "0b781a8fd2ef4038bdabbf19912844af",
            "f9a2579d20e349828914f2dddfab5373",
            "f9d616a66ec04ab7ad0fc83c1146233e",
            "0888e9466de242ac8279cf8dd2381225",
            "82e4cef75b784340b386678f7497b5e0",
            "0a411580842c431f9802e5c0505f2d93",
            "00cdd26ddcd2498592d8030a149ff583",
            "8012be24750a4690b0637ca6eda7aa5b",
            "1d461b2dbdd24176829423f7bdbb9e20",
            "c5ce5ef035344ac09515b09c30ab83e6",
            "d649322d87aa4784a4ae38fce7a76e9a",
            "ab82dc5b18834bfb8322b9eacc12def6",
            "1de89e421d1f4b3f879982f9b9078f2c",
            "15ccb12b8c5a460984d5770793280569",
            "65fd50c3dc934571a6214ad4cebc10be",
            "4ef5f97a01794c3d844394e7092902e8",
            "7282ea45f6da43e5ab3d14d835a772fb",
            "83bcdb3870104ce582456de039a4375d",
            "36e14ddb762e4c3186cbfcef38265d8e",
            "dd876484316b4bb299f5e0a9b070c4fc",
            "51fbf70ccf964c268f832a4b0c542fce",
            "27c1849720bb4293bfff8d8f28db395e",
            "8d5e78abc2da453ca7bb5f70da642531",
            "e5d451cb50314652a5b8a24905474d6a",
            "74f46d3025184777b8f586b4dd55a559",
            "90a84b5d5c2344b49a2dac358422dbdc",
            "afb3e413c99640a188ffa102f06b363e",
            "52b0f4d3fda64c78b9e42c9cdb81d1b9",
            "7406b433314f490c845e0e9d5b9871e1",
            "dca4ded604ec4a828e3d1da327099df7",
            "74266c093c6047f883f145a179d68db6",
            "d3f519517e69478dbaa50f5da3c28a46",
            "1f397967a05a469f9b1ca22095a991ed",
            "ce76977f05494c8d97ee7593ac57c4e0",
            "034bb9489af745969b18d29dbaa48149",
            "df061ef105334197b8ef6980847df43e",
            "b4d752d2cc6f4fea96e00bb76c7876d5",
            "4b332114d173425fb40bfd63383d206c",
            "91e2b0b020124db78545526b015e9184",
            "da2f946d5f014bfda52bc04fb3371dca",
            "b6bf133fec3347828951cfc3db367ba4",
            "8593b51d792a442ba14484c552e1d28f",
            "80c18aa7d8fa47c3a7b96a4523dee4b6",
            "0094ea6624ce4caa8709838cdcc394c1",
            "da8a3becb07a4872927a6cf69777cd6d",
            "2311195e154042e780bbb8d1f2436154",
            "4a3b844777b342f59ea4e1f603b88fa2",
            "fcbdb6b6879c48d2bb0f8e3c5ce94d07",
            "0095e4908c3f408e84bf33c8efe30595",
            "c3a55b4fb2994ab7a5a418bf628991ff",
            "a55fe47fddf44c849f9c013c74d92fd1",
            "43d3c9698be1485cb5cb5ac3a12f7e07",
            "439ad081bfd442198bf74c6179bc7f92",
            "78a89ab94cfb463baa4576d3fead3636",
            "6443054bc5b74d459b4e359b826092fb",
            "57a492f25b4d4daf913b8c2e31da91e5",
            "e47f246080d54c24b36f1e264f10c73f",
            "4330ab98045e490b8693b98a001aff25",
            "42de29738a52432aa78225875eabfa90",
            "0fa17177d0fd44a0832ced6a7f179e09",
            "38f2a902b1ec4512974a87aafe1cf22a",
            "fd5c3b512cd54491ae3b311b5fcff638",
            "67ecb59f8d3c40e6b4e6f0d6dcd096ed",
            "d8ab692ff1a940a9a4eb6d7991605a73",
            "5e0a8da8849e46dea0e959f64460fea9",
            "07c0ff818b554d3cbe3dbfa2051d3405",
            "a0c35e3a068344e5b660839c43aff449",
            "e3a46253b7924576bbf30f338c22dee3",
            "fe80c5d76e6c40179d6a6d2179f34b64",
            "619b79a7d283483d9459779b7a86f0cd",
            "1df2834204f145bcab369cd4254f3508",
            "ae6f1cb60b284122a1793d146409df86",
            "9fcd897849554e92844575a2c82d819c",
            "103ff80d0d934fe28ed82e1f6a392969",
            "484818439f5c489abbf48d29837c1d74",
            "b1b07fe790f749c6bd630072eebcfabd",
            "71834355db684a788f816466f9a7c081",
            "4140ac8c4bd24ff0992bfed63a5ffd8a",
            "d3c6bf88604a4de7b0ba4cee61cdb788",
            "a4f0d1a4e0b644dea41277f9986cda34",
            "e72dd2581aa543dcb72cdae16b283481",
            "a3efe6b81b8740fbbd0bdc0e16b41baf",
            "147c381050644d5a8cca0830488d5890",
            "733ad754146d4e13a0dbc903f46ea267",
            "f353b807399b45dd928db7d4163f56b9",
            "060d09b8bd024884a1e8e2cbec249fc5",
            "35ef7c840b274b6b8bbca0087ee7f707",
            "c9f86a7ec95849ce9b9e2848c8c42d25",
            "9360df619b5c4d799b23c3cb0237aed5",
            "371d0e84303849ec89e70b7c95e1a257"
          ]
        },
        "id": "RZ5iQ_KkJufJ",
        "outputId": "df140797-ec37-4156-b28a-769e76189960"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3cb088fe31a2479093e70161d0f5ae1c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1c4071860fe045dbae85b93f50334871",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b7a72b115b8a4d4cb7843db2488563be",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/94.8k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f9d616a66ec04ab7ad0fc83c1146233e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "15ccb12b8c5a460984d5770793280569",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "74f46d3025184777b8f586b4dd55a559",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "df061ef105334197b8ef6980847df43e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a3b844777b342f59ea4e1f603b88fa2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4330ab98045e490b8693b98a001aff25",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fe80c5d76e6c40179d6a6d2179f34b64",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a4f0d1a4e0b644dea41277f9986cda34",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from llama_index.core.prompts import PromptTemplate\n",
        "from llama_index.llms.together import TogetherLLM\n",
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "# Use the Together AI service to access the LLaMA2-70B chat model\n",
        "llm = TogetherLLM(\n",
        "    model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\",\n",
        "    api_key=os.environ[\"TOGETHER_AI_API_TOKEN\"]\n",
        ")\n",
        "\n",
        "# create index from vector store\n",
        "index = VectorStoreIndex.from_vector_store(vector_store, embed_model=\"local:BAAI/bge-small-en-v1.5\")\n",
        "\n",
        "# Define a query engine that is responsible for retrieving related pieces of text,\n",
        "# and using a LLM to formulate the final answer.\n",
        "query_engine = index.as_query_engine(llm=llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JPD8yAinVSq"
      },
      "source": [
        "# Test query engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWAI0jUhJ7qH",
        "outputId": "cfe45bce-3bc4-4717-9321-ee158e41c146"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Llama 2 is available in four different model sizes: 7 billion, 13 billion, 34 billion, and 70 billion parameters.\n"
          ]
        }
      ],
      "source": [
        "res = query_engine.query(\"How many parameters LLaMA2 has?\")\n",
        "print(res.response)\n",
        "# 'Llama 2 is available in four different model sizes: 7 billion, 13 billion, 34 billion, and 70 billion parameters.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvSmOtqBoCY2",
        "outputId": "36f670ef-9d77-4fbe-cb61-4fba5920cade"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Node ID\t f760111a-4acf-434a-b289-35c7b7de94dd\n",
            "Title\t Meta's Llama 2: Revolutionizing Open Source Language Models for Commercial Use\n",
            "Text\t I. Llama 2: Revolutionizing Commercial Use Unlike its predecessor Llama 1, which was limited to research use, Llama 2 represents a major advancement as an open-source commercial model. Businesses can now integrate Llama 2 into products to create AI-powered applications. Availability on Azure and AWS facilitates fine-tuning and adoption. However, restrictions apply to prevent exploitation. Companies with over 700 million active daily users cannot use Llama 2. Additionally, its output cannot be used to improve other language models.  II. Llama 2 Model Flavors Llama 2 is available in four different model sizes: 7 billion, 13 billion, 34 billion, and 70 billion parameters. While 7B, 13B, and 70B have already been released, the 34B model is still awaited. The pretrained variant, trained on a whopping 2 trillion tokens, boasts a context window of 4096 tokens, twice the size of its predecessor Llama 1. Meta also released a Llama 2 fine-tuned model for chat applications that was trained on over 1 million human annotations. Such extensive training comes at a cost, with the 70B model taking a staggering 1720320 GPU hours to train. The context window's length determines the amount of content the model can process at once, making Llama 2 a powerful language model in terms of scale and efficiency.  III. Safety Considerations: A Top Priority for Meta Meta's commitment to safety and alignment shines through in Llama 2's design. The model demonstrates exceptionally low AI safety violation percentages, surpassing even ChatGPT in safety benchmarks. Finding the right balance between helpfulness and safety when optimizing a model poses significant challenges. While a highly helpful model may be capable of answering any question, including sensitive ones like \"How do I build a bomb?\", it also raises concerns about potential misuse. Thus, striking the perfect equilibrium between providing useful information and ensuring safety is paramount. However, prioritizing safety to an extreme extent can lead to a model that struggles to effectively address a diverse range of questions. This limitation could hinder the model's practical applicability and user experience. Thus, achieving\n",
            "Score\t 0.6191229015079209\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Node ID\t 4679dd5f-8c98-469f-8e25-3bab8cd8433f\n",
            "Title\t Beyond GPT-4: What's New?\n",
            "Text\t LLM Variants and Meta's Open Source Before shedding light on four major trends, I'd share the latest Meta's Llama 2 and Code Llama. Meta's Llama 2 represents a sophisticated evolution in LLMs. This suite spans models pretrained and fine-tuned across a parameter spectrum of 7 billion to 70 billion. A specialized derivative, Llama 2-Chat, has been engineered explicitly for dialogue-centric applications. Benchmarking revealed Llama 2's superior performance over most extant open-source chat models. Human-centric evaluations, focusing on safety and utility metrics, positioned Llama 2-Chat as a potential contender against proprietary, closed-source counterparts. The development trajectory of Llama 2 emphasized rigorous fine-tuning methodologies. Meta's transparent delineation of these processes aims to catalyze community-driven advancements in LLMs, underscoring a commitment to collaborative and responsible AI development. Code Llama is built on top of Llama 2 and is available in three models: Code Llama, the foundational code model;Codel Llama - Python specialized for Python;and Code Llama - Instruct, which is fine-tuned for understanding natural language instructions. Based on its benchmark testing, Code Llama outperformed state-of-the-art publicly available LLMs (except GPT-4) on code tasks. Llama 2, Llama 2-Chat, and Code Llama are key steps in LLM development but still have a way to go compared to GPT-4. Meta's open access and commitment to improving these models promise transparent and faster LLM progress in the future. Please refer to the LLM and Llama variants below:  From LLMs to Multimodal LLMs, like OpenAI's ChatGPT (GPT-3.5), primarily focus on understanding and generating human language. They've been instrumental in tasks like text generation, translation, and even creative writing. However, their scope is limited to text. Enter multimodal models like GPT-4. These are a new breed of AI models that can understand and generate not just text, but also images, sounds, and potentially other types of data. The term \"multimodal\" refers to their ability to process multiple modes or\n",
            "Score\t 0.5952622757578383\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
          ]
        }
      ],
      "source": [
        "# print the source nodes used to write the answer\n",
        "for src in res.source_nodes:\n",
        "  print(\"Node ID\\t\", src.node_id)\n",
        "  print(\"Title\\t\", src.metadata['title'])\n",
        "  print(\"Text\\t\", src.text)\n",
        "  print(\"Score\\t\", src.score)\n",
        "  print(\"-_\"*20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMkpzH7vvb09"
      },
      "source": [
        "# Evaluate the retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8a3eKgKvckU",
        "outputId": "0ab007e9-4d7b-4eb4-f3c4-da0d5df45513"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 108/108 [01:59<00:00,  1.11s/it]\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core.evaluation import generate_question_context_pairs\n",
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "# Create questions for each segment. These questions will be used to\n",
        "# assess whether the retriever can accurately identify and return the\n",
        "# corresponding segment when queried.\n",
        "llm = OpenAI(model=\"gpt-4o-mini\")\n",
        "rag_eval_dataset = generate_question_context_pairs(\n",
        "    nodes,\n",
        "    llm=llm,\n",
        "    num_questions_per_chunk=1\n",
        ")\n",
        "\n",
        "# We can save the evaluation dataset as a json file for later use.\n",
        "rag_eval_dataset.save_json(\"./rag_eval_dataset.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjM95B9Zs29W"
      },
      "source": [
        "If you have uploaded the generated question JSON file, please uncomment the code in the next cell block. This will avoid the need to generate the questions manually, saving you time and effort."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sA1K84U254o"
      },
      "outputs": [],
      "source": [
        "#from llama_index.finetuning.embeddings.common import EmbeddingQAFinetuneDataset\n",
        "\n",
        "from llama_index.core.evaluation import EmbeddingQAFinetuneDataset\n",
        "\n",
        "rag_eval_dataset = EmbeddingQAFinetuneDataset.from_json(\n",
        "    \"./rag_eval_dataset.json\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNLxDxoc2-Ac"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "from llama_index.core.evaluation import RetrieverEvaluator, RelevancyEvaluator, FaithfulnessEvaluator, BatchEvalRunner\n",
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "async def run_evaluation(index, rag_eval_dataset, top_k_values, llm_judge,llm, n_queries_to_evaluate=20,num_work=1):\n",
        "    evaluation_results = {}\n",
        "\n",
        "    # ------------------- MRR and Hit Rate -------------------\n",
        "\n",
        "    for top_k in top_k_values:\n",
        "        # Get MRR and Hit Rate\n",
        "        retriever = index.as_retriever(similarity_top_k=top_k)\n",
        "        retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
        "            [\"mrr\", \"hit_rate\"], retriever=retriever\n",
        "        )\n",
        "        eval_results = await retriever_evaluator.aevaluate_dataset(rag_eval_dataset)\n",
        "        avg_mrr = sum(res.metric_vals_dict[\"mrr\"] for res in eval_results) / len(eval_results)\n",
        "        avg_hit_rate = sum(res.metric_vals_dict[\"hit_rate\"] for res in eval_results) / len(eval_results)\n",
        "\n",
        "        # Collect the evaluation results\n",
        "        evaluation_results[f\"mrr_@_{top_k}\"] = avg_mrr\n",
        "        evaluation_results[f\"hit_rate_@_{top_k}\"] = avg_hit_rate\n",
        "\n",
        "    # ------------------- Faithfulness and Relevancy -------------------\n",
        "\n",
        "    # Extract the questions from the dataset\n",
        "    queries = list(rag_eval_dataset.queries.values())\n",
        "    batch_eval_queries = queries[:n_queries_to_evaluate]\n",
        "\n",
        "    # Initiate the faithfulnes and relevancy evaluator objects\n",
        "    faithfulness_evaluator = FaithfulnessEvaluator(llm=llm_judge)\n",
        "    relevancy_evaluator = RelevancyEvaluator(llm=llm_judge)\n",
        "\n",
        "    # The batch evaluator runs the evaluation in batches\n",
        "    runner = BatchEvalRunner(\n",
        "        {\n",
        "            \"faithfulness\": faithfulness_evaluator,\n",
        "            \"relevancy\": relevancy_evaluator\n",
        "        },\n",
        "        workers=num_work,\n",
        "        show_progress=True,\n",
        "    )\n",
        "\n",
        "    # Get faithfulness and relevancy scores\n",
        "    query_engine = index.as_query_engine(llm=llm)\n",
        "    eval_results = await runner.aevaluate_queries(\n",
        "        query_engine, queries=batch_eval_queries\n",
        "    )\n",
        "    faithfulness_score = sum(result.passing for result in eval_results['faithfulness']) / len(eval_results['faithfulness'])\n",
        "    relevancy_score = sum(result.passing for result in eval_results['relevancy']) / len(eval_results['relevancy'])\n",
        "    evaluation_results[\"faithfulness\"] = faithfulness_score\n",
        "    evaluation_results[\"relevancy\"] = relevancy_score\n",
        "\n",
        "    return evaluation_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPyUTGk6c5r3",
        "outputId": "e8fe9f59-72f5-4efe-920a-d9895600cc3f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [01:12<00:00,  3.62s/it]\n",
            "100%|██████████| 40/40 [00:40<00:00,  1.02s/it]\n"
          ]
        }
      ],
      "source": [
        "# We evaluate the retrievers with different top_k values.\n",
        "top_k_values = [2, 4, 6, 8, 10]\n",
        "\n",
        "llm_judge = OpenAI(temperature=0, model=\"gpt-4o\")\n",
        "\n",
        "evaluation_results = await run_evaluation(index, rag_eval_dataset, top_k_values, llm_judge,llm=llm,n_queries_to_evaluate=20,num_work=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5QO_qngc5r4",
        "outputId": "46cc2c93-d83c-40af-c593-aa6aa4f83c8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'mrr_@_2': 0.7870370370370371, 'hit_rate_@_2': 0.8703703703703703, 'mrr_@_4': 0.8016975308641976, 'hit_rate_@_4': 0.9259259259259259, 'mrr_@_6': 0.8050925925925926, 'hit_rate_@_6': 0.9444444444444444, 'mrr_@_8': 0.8077380952380954, 'hit_rate_@_8': 0.9629629629629629, 'mrr_@_10': 0.8077380952380954, 'hit_rate_@_10': 0.9629629629629629, 'faithfulness': 1.0, 'relevancy': 1.0}\n"
          ]
        }
      ],
      "source": [
        "print(evaluation_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwpezMYKc5r4",
        "outputId": "ee789748-baef-48a5-b50b-14e3b4706969"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:07<00:00,  2.60it/s]\n",
            "100%|██████████| 40/40 [00:06<00:00,  6.30it/s]\n"
          ]
        }
      ],
      "source": [
        "# Use GPT-4o-mini as the LLM model\n",
        "llm = OpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "# run evaluation with GPT-4o-mini\n",
        "top_k_values = [2, 4, 6, 8, 10]\n",
        "llm_judge = OpenAI(temperature=0, model=\"gpt-4o\")\n",
        "evaluation_results = await run_evaluation(index, rag_eval_dataset, top_k_values, llm_judge,llm=llm, n_queries_to_evaluate=20,num_work=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhKNYtxSc5r5",
        "outputId": "d5ce5b99-71dc-4570-8157-499fd158fe7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'mrr_@_2': 0.7870370370370371, 'hit_rate_@_2': 0.8703703703703703, 'mrr_@_4': 0.8016975308641976, 'hit_rate_@_4': 0.9259259259259259, 'mrr_@_6': 0.8050925925925926, 'hit_rate_@_6': 0.9444444444444444, 'mrr_@_8': 0.8077380952380954, 'hit_rate_@_8': 0.9629629629629629, 'mrr_@_10': 0.8077380952380954, 'hit_rate_@_10': 0.9629629629629629, 'faithfulness': 1.0, 'relevancy': 1.0}\n"
          ]
        }
      ],
      "source": [
        "print(evaluation_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iq3MSK4Wc5r5"
      },
      "outputs": [],
      "source": [
        "# from llama_index.llms.gemini import Gemini\n",
        "\n",
        "# # Use Gemini as the LLM model\n",
        "# llm = Gemini(model=\"models/gemini-1.5-flash\")\n",
        "\n",
        "# # run evaluation with Gemini\n",
        "# top_k_values = [2, 4, 6, 8, 10]\n",
        "# llm_judge = OpenAI(temperature=0, model=\"gpt-4o\")\n",
        "# evaluation_results = await run_evaluation(index, rag_eval_dataset, top_k_values, llm_judge,llm=llm, n_queries_to_evaluate=20,num_work=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YU-VMiVQc5r6"
      },
      "source": [
        "# Inference speed comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvzTTqOPc5r7"
      },
      "outputs": [],
      "source": [
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Y0UY-qAc5r7",
        "outputId": "fb3cfa92-b898-4731-d0d8-f3ddb6069bdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time taken for Llama 3.1 70B on Together AI: 2.44 seconds\n"
          ]
        }
      ],
      "source": [
        "llm = TogetherLLM(\n",
        "    model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\",\n",
        "    api_key=os.environ[\"TOGETHER_AI_API_TOKEN\"]\n",
        ")\n",
        "\n",
        "time_start = time.time()\n",
        "llm.complete(\"List the 50 states in the United States of America. Write their names in a comma-separated list and nothing else.\")\n",
        "time_end = time.time()\n",
        "print(\"Time taken for Llama 3.1 70B on Together AI: {0:.2f} seconds\".format(time_end - time_start))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yT01WxJ-c5r8",
        "outputId": "eee8dde2-fda8-406e-cf35-893a04b27793"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time taken for GPT 4o Mini: 2.80 seconds\n"
          ]
        }
      ],
      "source": [
        "llm = OpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "time_start = time.time()\n",
        "llm.complete(\"List the 50 states in the United States of America. Write their names in a comma-separated list and nothing else.\")\n",
        "time_end = time.time()\n",
        "print(\"Time taken for GPT 4o Mini: {0:.2f} seconds\".format(time_end - time_start))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_y3dy66zc5r8",
        "outputId": "65996a28-a3f0-464d-8618-c4a17a5801d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time taken for Gemini 1.5 Flash: 2.69 seconds\n"
          ]
        }
      ],
      "source": [
        "from llama_index.llms.gemini import Gemini\n",
        "\n",
        "llm = Gemini(model=\"models/gemini-1.5-flash\")\n",
        "\n",
        "time_start = time.time()\n",
        "llm.complete(\"List the 50 states in the United States of America. Write their names in a comma-separated list and nothing else.\")\n",
        "time_end = time.time()\n",
        "print(\"Time taken for Gemini 1.5 Flash: {0:.2f} seconds\".format(time_end - time_start))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baIX_beng-Lo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "venv_ai_tutor",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}