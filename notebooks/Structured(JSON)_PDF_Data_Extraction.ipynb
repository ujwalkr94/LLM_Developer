{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ujwalkr94/LLM_Developer/blob/main/notebooks/Structured(JSON)_PDF_Data_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rCndQqynkNiQ",
        "outputId": "f00d4316-7edc-42f9-b907-153f37218930"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q google-generativeai==0.8.3 openai==1.59.6 tiktoken==0.8.0 llama-index-llms-gemini==0.4.4 arxiv==2.1.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PdAwk6gTsDA3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Set the \"OPENAI_API_KEY\" in the Python environment. Will be used by OpenAI client later.\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"[OPENAI_API_KEY]\"\n",
        "\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kk_QGIJLT8qY"
      },
      "source": [
        "### OpenAI Structured Output without `response_format`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_F_PrQCT6lp"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gaCYqKDrT6Va",
        "outputId": "8075f56f-7bb4-4491-e69c-bd4869273ed8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"Top10BestSellingBooks\": [\n",
            "    {\n",
            "      \"title\": \"Don Quixote\",\n",
            "      \"author\": \"Miguel de Cervantes\",\n",
            "      \"yearPublished\": \"1605\",\n",
            "      \"summary\": \"A Spanish novel about the adventures of a nobleman who reads so many chivalric romances that he loses his sanity and decides to become a knight-errant.\"\n",
            "    },\n",
            "    {\n",
            "      \"title\": \"A Tale of Two Cities\",\n",
            "      \"author\": \"Charles Dickens\",\n",
            "      \"yearPublished\": \"1859\",\n",
            "      \"summary\": \"A historical novel set in London and Paris before and during the French Revolution, focusing on themes of resurrection and transformation.\"\n",
            "    },\n",
            "    {\n",
            "      \"title\": \"The Lord of the Rings\",\n",
            "      \"author\": \"J.R.R. Tolkien\",\n",
            "      \"yearPublished\": \"1954\",\n",
            "      \"summary\": \"An epic fantasy novel that follows the quest to destroy the One Ring and defeat the Dark Lord Sauron.\"\n",
            "    },\n",
            "    {\n",
            "      \"title\": \"The Little Prince\",\n",
            "      \"author\": \"Antoine de Saint-Exupéry\",\n",
            "      \"yearPublished\": \"1943\",\n",
            "      \"summary\": \"A philosophical tale about a young prince who travels from planet to planet, learning about life and human nature.\"\n",
            "    },\n",
            "    {\n",
            "      \"title\": \"Harry Potter and the Philosopher's Stone\",\n",
            "      \"author\": \"J.K. Rowling\",\n",
            "      \"yearPublished\": \"1997\",\n",
            "      \"summary\": \"The first book in the Harry Potter series, introducing Harry as he discovers his magical heritage and attends Hogwarts School of Witchcraft and Wizardry.\"\n",
            "    },\n",
            "    {\n",
            "      \"title\": \"The Hobbit\",\n",
            "      \"author\": \"J.R.R. Tolkien\",\n",
            "      \"yearPublished\": \"1937\",\n",
            "      \"summary\": \"A fantasy novel about the journey of Bilbo Baggins, a hobbit who is reluctantly drawn into an epic quest to reclaim a treasure guarded by a dragon.\"\n",
            "    },\n",
            "    {\n",
            "      \"title\": \"And Then There Were None\",\n",
            "      \"author\": \"Agatha Christie\",\n",
            "      \"yearPublished\": \"1939\",\n",
            "      \"summary\": \"A mystery novel in which ten strangers are invited to an isolated island, where they are killed one by one.\"\n",
            "    },\n",
            "    {\n",
            "      \"title\": \"Dream of the Red Chamber\",\n",
            "      \"author\": \"Cao Xueqin\",\n",
            "      \"yearPublished\": \"1791\",\n",
            "      \"summary\": \"A Chinese novel that provides a detailed, episodic record of the lives of two branches of the wealthy, aristocratic Jia clan.\"\n",
            "    },\n",
            "    {\n",
            "      \"title\": \"The Lion, the Witch and the Wardrobe\",\n",
            "      \"author\": \"C.S. Lewis\",\n",
            "      \"yearPublished\": \"1950\",\n",
            "      \"summary\": \"A fantasy novel about four children who discover a magical land called Narnia, which is under the spell of an evil witch.\"\n",
            "    },\n",
            "    {\n",
            "      \"title\": \"The Da Vinci Code\",\n",
            "      \"author\": \"Dan Brown\",\n",
            "      \"yearPublished\": \"2003\",\n",
            "      \"summary\": \"A mystery thriller that follows symbologist Robert Langdon and cryptologist Sophie Neveu as they investigate a murder in the Louvre Museum.\"\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "system_prompt = \"\"\"\n",
        "You are a helpful assistant designed to output information exclusively in JSON format. Your response should contain only raw JSON data with no additional text, explanations, or comments. Do not include backticks (`) or any code block delimiters in your response.\n",
        "\n",
        "Always use the key `\"Top10BestSellingBooks\"` when listing top 10 best-selling books. Follow the specified JSON structure below:\n",
        "\n",
        "### JSON Format Example\n",
        "{\n",
        "  \"Top10BestSellingBooks\": [\n",
        "    {\n",
        "      \"title\": \"Book Title\",\n",
        "      \"author\": \"Author Name\",\n",
        "      \"yearPublished\": \"Year\",\n",
        "      \"summary\": \"Brief summary of the book.\"\n",
        "    },\n",
        "    {\n",
        "      \"title\": \"Book Title 2\",\n",
        "      \"author\": \"Author Name 2\",\n",
        "      \"yearPublished\": \"Year\",\n",
        "      \"summary\": \"Brief summary of the book.\"\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "Always respond with clean JSON output that can be directly used with JSON parsers like `json.loads()`.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "prompt = \"Give me the names of the 10 best-selling books, their authors, the year they were published, and a concise summary in JSON format\"\n",
        "\n",
        "\n",
        "# Making the API call\n",
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-4o-2024-08-06\",\n",
        "  temperature = 0,\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\":system_prompt},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "  ]\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hY1nUQ8QT6Rv",
        "outputId": "9451a327-1431-4f1a-cb47-893c06b5c1da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'dict'>\n",
            "{'title': 'Don Quixote', 'author': 'Miguel de Cervantes', 'yearPublished': '1605', 'summary': 'A Spanish novel about the adventures of a nobleman who reads so many chivalric romances that he loses his sanity and decides to become a knight-errant.'}\n",
            "-------------------------------------\n",
            "Don Quixote\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "result_book = json.loads(response.choices[0].message.content)\n",
        "\n",
        "print(type(result_book))\n",
        "\n",
        "print(result_book['Top10BestSellingBooks'][0])\n",
        "print(\"-------------------------------------\")\n",
        "print(result_book['Top10BestSellingBooks'][0]['title'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZpYV-q0--QB"
      },
      "source": [
        "## OpenAI Strucutred output (JSON) with `response_format`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iH1CDNr30dtU"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8702bCYi9xpy"
      },
      "outputs": [],
      "source": [
        "prompt = \"Give me the names of the 10 best-selling books, their authors, the year they were published, and a concise summary in JSON format\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlCRdRTQ9FNh"
      },
      "outputs": [],
      "source": [
        "# The response format- JSON schema\n",
        "response_format_json = {\n",
        "  \"type\": \"json_schema\",\n",
        "  \"json_schema\": {\n",
        "    \"name\": \"Top10BestSellingBooks\",\n",
        "    \"strict\": True,\n",
        "    \"schema\": {\n",
        "      \"type\": \"object\",\n",
        "      \"properties\": {\n",
        "        \"Top10BestSellingBooks\": {\n",
        "          \"type\": \"array\",\n",
        "          \"items\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "              \"title\": { \"type\": \"string\" },\n",
        "              \"author\": { \"type\": \"string\" },\n",
        "              \"yearPublished\": { \"type\": \"integer\" },\n",
        "              \"summary\": { \"type\": \"string\" }\n",
        "            },\n",
        "\n",
        "            \"required\": [\"title\", \"author\", \"yearPublished\", \"summary\"],\n",
        "            \"additionalProperties\": False\n",
        "          }\n",
        "        }\n",
        "      },\n",
        "      \"required\": [\"Top10BestSellingBooks\"],\n",
        "      \"additionalProperties\": False\n",
        "    }\n",
        "  }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwcVCEOExNuL"
      },
      "outputs": [],
      "source": [
        "system_prompt = \"\"\"\n",
        "You are a helpful assistant designed to output information exclusively in JSON format.\n",
        "### JSON Format Example\n",
        "{\n",
        "  \"Top10BestSellingBooks\": [\n",
        "    {\n",
        "      \"title\": \"Book Title\",\n",
        "      \"author\": \"Author Name\",\n",
        "      \"yearPublished\": \"Year\",\n",
        "      \"summary\": \"Brief summary of the book.\"\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kY_AfQvR91Ei",
        "outputId": "e029e38a-4e85-456e-9d8f-3144194a2ca1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\"Top10BestSellingBooks\":[{\"title\":\"Where the Crawdads Sing\",\"author\":\"Delia Owens\",\"yearPublished\":2018,\"summary\":\"A coming-of-age story about a young girl named Kya who grows up isolated in the marshes of North Carolina and becomes a brilliant naturalist.\"},{\"title\":\"The Silent Patient\",\"author\":\"Alex Michaelides\",\"yearPublished\":2019,\"summary\":\"A psychological thriller about a woman who stops speaking after allegedly murdering her husband, and the therapist determined to uncover her motive.\"},{\"title\":\"The Vanishing Half\",\"author\":\"Brit Bennett\",\"yearPublished\":2020,\"summary\":\"A multi-generational narrative about twin sisters who choose to live in two very different worlds, one black and one white.\"},{\"title\":\"The Midnight Library\",\"author\":\"Matt Haig\",\"yearPublished\":2020,\"summary\":\"A novel about a library that allows a woman to explore different versions of her life, leading her to discover what truly makes life worth living.\"},{\"title\":\"The Four Winds\",\"author\":\"Kristin Hannah\",\"yearPublished\":2021,\"summary\":\"A historical fiction novel set during the Great Depression, focusing on a woman's struggle to keep her family together during hard times.\"},{\"title\":\"The Last Thing He Told Me\",\"author\":\"Laura Dave\",\"yearPublished\":2021,\"summary\":\"A mystery about a woman whose husband disappears, leaving behind a note asking her to protect his daughter, leading to a search for the truth.\"},{\"title\":\"It Ends with Us\",\"author\":\"Colleen Hoover\",\"yearPublished\":2016,\"summary\":\"A romance novel that explores the complexities of love and the difficult choices that come with it, focusing on a young woman named Lily.\"},{\"title\":\"Atomic Habits\",\"author\":\"James Clear\",\"yearPublished\":2018,\"summary\":\"A self-help book that provides practical strategies for forming good habits, breaking bad ones, and mastering the tiny behaviors that lead to remarkable results.\"},{\"title\":\"Becoming\",\"author\":\"Michelle Obama\",\"yearPublished\":2018,\"summary\":\"A memoir by the former First Lady of the United States, chronicling her life from childhood through her years in the White House.\"},{\"title\":\"Educated\",\"author\":\"Tara Westover\",\"yearPublished\":2018,\"summary\":\"A memoir about a woman who grows up in a strict and abusive household in rural Idaho but eventually escapes to learn about the wider world through education.\"}]}\n"
          ]
        }
      ],
      "source": [
        "# Making the API call\n",
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-4o-2024-08-06\",\n",
        "  response_format=response_format_json,\n",
        "  temperature = 0,\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\":system_prompt},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "  ]\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lY4mHR4wQEzq",
        "outputId": "12748daf-d228-45a5-9f9b-8b334eca9dbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'str'>\n"
          ]
        }
      ],
      "source": [
        "print(type(response.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_A_XRmV2VkU",
        "outputId": "23f8e827-db87-40af-a6cd-f8a039befa05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'dict'>\n",
            "{'title': 'Where the Crawdads Sing', 'author': 'Delia Owens', 'yearPublished': 2018, 'summary': 'A coming-of-age story about a young girl named Kya who grows up isolated in the marshes of North Carolina and becomes a brilliant naturalist.'}\n",
            "-------------------------------------\n",
            "Where the Crawdads Sing\n"
          ]
        }
      ],
      "source": [
        "result_book = json.loads(response.choices[0].message.content)\n",
        "\n",
        "print(type(result_book))\n",
        "\n",
        "print(result_book['Top10BestSellingBooks'][0])\n",
        "print(\"-------------------------------------\")\n",
        "print(result_book['Top10BestSellingBooks'][0]['title'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nISPPdFA1WvX"
      },
      "source": [
        "## Strucutred output from PDF + OpenAI + pdf2images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "HGxeZqCQ28bi",
        "outputId": "ebc04611-e58e-4a2c-b385-131772d95618"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 696 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.5 [186 kB]\n",
            "Fetched 186 kB in 0s (1,015 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 123623 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.5_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.5) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.5) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U pdf2image\n",
        "!apt-get install poppler-utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "8b7d9cd648a04f5c82127ceebd82df6a",
            "45afc5598f334aa4977b523b9005769a",
            "5118b5001d89446cbcd7f6ba1f7f1174",
            "a3b5d4d1f0b54618b00cc64647d4cb67",
            "a2d7037593544be5967063e52c864e42",
            "589810654f564e339668f2b356bf476e",
            "343acbd7b5a44c84901e4b61096f92c5",
            "9aa872b1f4a94e11b7ba040db2b960bf",
            "71e1cec2759a4268b346e2eeb45b8ad4",
            "9e8da6486b314032b6c25c6785757b12",
            "82cc4f36aa74451f9972824473bbaa69"
          ]
        },
        "id": "n3QpOuoOiMvB",
        "outputId": "ba82fbfd-108c-4b43-cf74-536c4b4e0939"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8b7d9cd648a04f5c82127ceebd82df6a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "rag_research_paper.zip:   0%|          | 0.00/7.51M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "file_path = hf_hub_download(repo_id=\"jaiganesan/ai_tutor_knowledge\", filename=\"rag_research_paper.zip\",repo_type=\"dataset\",local_dir=\"/content\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgQt32r8V50j",
        "outputId": "f3ffa060-9dc7-4bce-d49d-162ffbe3ca32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/rag_research_paper.zip\n",
            "   creating: /content/rag_research_paper/\n",
            "  inflating: /content/rag_research_paper/2405.07437v2.pdf  \n",
            "  inflating: /content/rag_research_paper/2407.01219v1.pdf  \n",
            "  inflating: /content/rag_research_paper/2407.07858v1.pdf  \n",
            "  inflating: /content/rag_research_paper/2407.08223v1.pdf  \n",
            "  inflating: /content/rag_research_paper/2407.16833v1.pdf  \n",
            "  inflating: /content/rag_research_paper/2407.21712v1.pdf  \n",
            "  inflating: /content/rag_research_paper/2408.08067v2.pdf  \n",
            "  inflating: /content/rag_research_paper/2408.08921v1.pdf  \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/rag_research_paper.zip -d /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lFWZkMuaZHS",
        "outputId": "31b42dfa-e4d6-4f5a-ba56-857ba5d0512c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['/content/pages/2407.08223v1/page-001.png', '/content/pages/2407.08223v1/page-002.png', '/content/pages/2407.08223v1/page-003.png', '/content/pages/2407.08223v1/page-004.png', '/content/pages/2407.08223v1/page-005.png', '/content/pages/2407.08223v1/page-006.png', '/content/pages/2407.08223v1/page-007.png', '/content/pages/2407.08223v1/page-008.png', '/content/pages/2407.08223v1/page-009.png', '/content/pages/2407.08223v1/page-010.png', '/content/pages/2407.08223v1/page-011.png', '/content/pages/2407.08223v1/page-012.png', '/content/pages/2407.08223v1/page-013.png', '/content/pages/2407.08223v1/page-014.png', '/content/pages/2407.08223v1/page-015.png', '/content/pages/2407.08223v1/page-016.png', '/content/pages/2407.08223v1/page-017.png', '/content/pages/2407.16833v1/page-001.png', '/content/pages/2407.16833v1/page-002.png', '/content/pages/2407.16833v1/page-003.png', '/content/pages/2407.16833v1/page-004.png', '/content/pages/2407.16833v1/page-005.png', '/content/pages/2407.16833v1/page-006.png', '/content/pages/2407.16833v1/page-007.png', '/content/pages/2407.16833v1/page-008.png', '/content/pages/2407.16833v1/page-009.png', '/content/pages/2407.16833v1/page-010.png', '/content/pages/2407.16833v1/page-011.png', '/content/pages/2407.16833v1/page-012.png', '/content/pages/2408.08921v1/page-001.png', '/content/pages/2408.08921v1/page-002.png', '/content/pages/2408.08921v1/page-003.png', '/content/pages/2408.08921v1/page-004.png', '/content/pages/2408.08921v1/page-005.png', '/content/pages/2408.08921v1/page-006.png', '/content/pages/2408.08921v1/page-007.png', '/content/pages/2408.08921v1/page-008.png', '/content/pages/2408.08921v1/page-009.png', '/content/pages/2408.08921v1/page-010.png', '/content/pages/2408.08921v1/page-011.png', '/content/pages/2408.08921v1/page-012.png', '/content/pages/2408.08921v1/page-013.png', '/content/pages/2408.08921v1/page-014.png', '/content/pages/2408.08921v1/page-015.png', '/content/pages/2408.08921v1/page-016.png', '/content/pages/2408.08921v1/page-017.png', '/content/pages/2408.08921v1/page-018.png', '/content/pages/2408.08921v1/page-019.png', '/content/pages/2408.08921v1/page-020.png', '/content/pages/2408.08921v1/page-021.png', '/content/pages/2408.08921v1/page-022.png', '/content/pages/2408.08921v1/page-023.png', '/content/pages/2408.08921v1/page-024.png', '/content/pages/2408.08921v1/page-025.png', '/content/pages/2408.08921v1/page-026.png', '/content/pages/2408.08921v1/page-027.png', '/content/pages/2408.08921v1/page-028.png', '/content/pages/2408.08921v1/page-029.png', '/content/pages/2408.08921v1/page-030.png', '/content/pages/2408.08921v1/page-031.png', '/content/pages/2408.08921v1/page-032.png', '/content/pages/2408.08921v1/page-033.png', '/content/pages/2408.08921v1/page-034.png', '/content/pages/2408.08921v1/page-035.png', '/content/pages/2408.08921v1/page-036.png', '/content/pages/2408.08921v1/page-037.png', '/content/pages/2408.08921v1/page-038.png', '/content/pages/2408.08921v1/page-039.png', '/content/pages/2408.08921v1/page-040.png', '/content/pages/2407.07858v1/page-001.png', '/content/pages/2407.07858v1/page-002.png', '/content/pages/2407.07858v1/page-003.png', '/content/pages/2407.07858v1/page-004.png', '/content/pages/2407.07858v1/page-005.png', '/content/pages/2407.07858v1/page-006.png', '/content/pages/2407.07858v1/page-007.png', '/content/pages/2407.07858v1/page-008.png', '/content/pages/2408.08067v2/page-001.png', '/content/pages/2408.08067v2/page-002.png', '/content/pages/2408.08067v2/page-003.png', '/content/pages/2408.08067v2/page-004.png', '/content/pages/2408.08067v2/page-005.png', '/content/pages/2408.08067v2/page-006.png', '/content/pages/2408.08067v2/page-007.png', '/content/pages/2408.08067v2/page-008.png', '/content/pages/2408.08067v2/page-009.png', '/content/pages/2408.08067v2/page-010.png', '/content/pages/2408.08067v2/page-011.png', '/content/pages/2408.08067v2/page-012.png', '/content/pages/2408.08067v2/page-013.png', '/content/pages/2408.08067v2/page-014.png', '/content/pages/2408.08067v2/page-015.png', '/content/pages/2408.08067v2/page-016.png', '/content/pages/2408.08067v2/page-017.png', '/content/pages/2408.08067v2/page-018.png', '/content/pages/2408.08067v2/page-019.png', '/content/pages/2408.08067v2/page-020.png', '/content/pages/2408.08067v2/page-021.png', '/content/pages/2408.08067v2/page-022.png', '/content/pages/2408.08067v2/page-023.png', '/content/pages/2408.08067v2/page-024.png', '/content/pages/2408.08067v2/page-025.png', '/content/pages/2408.08067v2/page-026.png', '/content/pages/2408.08067v2/page-027.png', '/content/pages/2407.21712v1/page-001.png', '/content/pages/2407.21712v1/page-002.png', '/content/pages/2407.21712v1/page-003.png', '/content/pages/2407.21712v1/page-004.png', '/content/pages/2407.21712v1/page-005.png', '/content/pages/2407.21712v1/page-006.png', '/content/pages/2407.21712v1/page-007.png', '/content/pages/2407.21712v1/page-008.png', '/content/pages/2407.21712v1/page-009.png', '/content/pages/2407.21712v1/page-010.png', '/content/pages/2407.21712v1/page-011.png', '/content/pages/2407.21712v1/page-012.png', '/content/pages/2405.07437v2/page-001.png', '/content/pages/2405.07437v2/page-002.png', '/content/pages/2405.07437v2/page-003.png', '/content/pages/2405.07437v2/page-004.png', '/content/pages/2405.07437v2/page-005.png', '/content/pages/2405.07437v2/page-006.png', '/content/pages/2405.07437v2/page-007.png', '/content/pages/2405.07437v2/page-008.png', '/content/pages/2405.07437v2/page-009.png', '/content/pages/2405.07437v2/page-010.png', '/content/pages/2405.07437v2/page-011.png', '/content/pages/2405.07437v2/page-012.png', '/content/pages/2405.07437v2/page-013.png', '/content/pages/2405.07437v2/page-014.png', '/content/pages/2405.07437v2/page-015.png', '/content/pages/2405.07437v2/page-016.png', '/content/pages/2405.07437v2/page-017.png', '/content/pages/2405.07437v2/page-018.png', '/content/pages/2405.07437v2/page-019.png', '/content/pages/2405.07437v2/page-020.png', '/content/pages/2405.07437v2/page-021.png', '/content/pages/2407.01219v1/page-001.png', '/content/pages/2407.01219v1/page-002.png', '/content/pages/2407.01219v1/page-003.png', '/content/pages/2407.01219v1/page-004.png', '/content/pages/2407.01219v1/page-005.png', '/content/pages/2407.01219v1/page-006.png', '/content/pages/2407.01219v1/page-007.png', '/content/pages/2407.01219v1/page-008.png', '/content/pages/2407.01219v1/page-009.png', '/content/pages/2407.01219v1/page-010.png', '/content/pages/2407.01219v1/page-011.png', '/content/pages/2407.01219v1/page-012.png', '/content/pages/2407.01219v1/page-013.png', '/content/pages/2407.01219v1/page-014.png', '/content/pages/2407.01219v1/page-015.png', '/content/pages/2407.01219v1/page-016.png', '/content/pages/2407.01219v1/page-017.png', '/content/pages/2407.01219v1/page-018.png', '/content/pages/2407.01219v1/page-019.png', '/content/pages/2407.01219v1/page-020.png', '/content/pages/2407.01219v1/page-021.png', '/content/pages/2407.01219v1/page-022.png']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pdf2image import convert_from_path\n",
        "\n",
        "pdf_directory = \"/content/rag_research_paper\"\n",
        "output_dir = \"/content/pages\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "pages_png = []\n",
        "\n",
        "for pdf_file in os.listdir(pdf_directory):\n",
        "    if pdf_file.endswith(\".pdf\"):\n",
        "        pdf_path = os.path.join(pdf_directory, pdf_file)\n",
        "\n",
        "        convert = convert_from_path(pdf_path, use_pdftocairo=True)\n",
        "\n",
        "        pdf_output_dir = os.path.join(output_dir, os.path.splitext(pdf_file)[0])\n",
        "        os.makedirs(pdf_output_dir, exist_ok=True)\n",
        "\n",
        "        for page_num, image in enumerate(convert):\n",
        "            page_filename = f\"page-{str(page_num + 1).zfill(3)}.png\"\n",
        "            full_path = os.path.join(pdf_output_dir, page_filename)\n",
        "            image.save(full_path)\n",
        "\n",
        "            pages_png.append(full_path)\n",
        "\n",
        "print(pages_png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eo5qbcZ6-IHN"
      },
      "outputs": [],
      "source": [
        "from io import BytesIO\n",
        "import base64\n",
        "import json\n",
        "\n",
        "# Function to encode the image\n",
        "\n",
        "def encode_image(image_path):\n",
        "\n",
        "  with open(image_path, \"rb\") as image_file:\n",
        "    return base64.b64encode(image_file.read()).decode('utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1Ogn8R7E5WZ"
      },
      "outputs": [],
      "source": [
        "system_instruction_prompt =\"\"\"\n",
        "You are an expert in extracting structured data from research paper images.\n",
        "\n",
        "Task Description:\n",
        "Extract comprehensive information from PDF research paper images, including all headlines, content, and visual elements.\n",
        "Preserve complete information without fragmentation.\n",
        "\n",
        "Must Follow Guidenline: Extract all text and information accurately from each image provided. Organize content into multiple\n",
        "JSON objects when appropriate, based on the amount and type of content. Each JSON should clearly reflect distinct content\n",
        "sections for streamlined analysis\n",
        "\n",
        "Content Requirements:\n",
        "1. Missing Headlines\n",
        "- If no visible headline exists, generate appropriate ones based on content\n",
        "- Group related content under these generated headlines\n",
        "\n",
        "2. Visual Elements\n",
        "For figures, graphs, tables, and architectures:\n",
        "- Extract title/caption\n",
        "- Describe main trends and comparisons\n",
        "- Detail architecture designs\n",
        "- Include related insights from surrounding text\n",
        "\n",
        "3. Text Processing\n",
        "- Extract complete sentences without summarization\n",
        "- Maintain original detail level\n",
        "- Merge fragmented content logically\n",
        "- Preserve all technical information\n",
        "\n",
        "Required output Format (JSON):\n",
        "[\n",
        "{\n",
        "    \"source\": \"Extract complete arXiv ID including prefix (e.g., arXiv:2405.07437v2).\n",
        "               Verify ID accuracy multiple times. if there is no Arxiv ID return None\",\n",
        "\n",
        "    \"name\": \"Extract or generate all headlines and subheadlines (e.g., Abstract,\n",
        "            Introduction, Methods, etc). Include section titles and subsection headings.\",\n",
        "\n",
        "    \"content\": \"For each section:\n",
        "                - Complete text content\n",
        "                - Visual element descriptions\n",
        "                - Figure/graph details:\n",
        "                  * Title/caption\n",
        "                  * Description\n",
        "                  * Key trends/comparisons\n",
        "                  * Architecture details\n",
        "                  * Related insights\"\n",
        "},\n",
        "]\n",
        "\n",
        "Key Guidelines:\n",
        "- Extract exact content without summarization\n",
        "- Ensure accuracy in complex technical details\n",
        "- Maintain logical content organization\n",
        "- Include complete visual element analysis\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPKc_1JkwQfd"
      },
      "outputs": [],
      "source": [
        "# The response format- JSON schema\n",
        "json_response_format = {\n",
        "  \"type\": \"json_schema\",\n",
        "  \"json_schema\": {\n",
        "    \"name\": \"research_paper_data\",\n",
        "    \"strict\": True,\n",
        "    \"schema\": {\n",
        "      \"type\": \"object\",\n",
        "      \"properties\": {\n",
        "        \"research_paper_data\": {\n",
        "          \"type\": \"array\",\n",
        "          \"items\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"name\": { \"type\": \"string\" },\n",
        "                \"source\": { \"type\": \"string\" },\n",
        "                \"content\": { \"type\": \"string\"},\n",
        "\n",
        "            },\n",
        "\n",
        "            \"required\": [\"name\", \"source\", \"content\"],\n",
        "            \"additionalProperties\": False\n",
        "          }\n",
        "        },\n",
        "      },\n",
        "      \"required\": [\"research_paper_data\"],\n",
        "      \"additionalProperties\": False\n",
        "    }\n",
        "  }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcyxZ3hee4O2"
      },
      "outputs": [],
      "source": [
        "import arxiv\n",
        "import re\n",
        "\n",
        "def arxiv_extraction(arxiv_id):\n",
        "  client = arxiv.Client()\n",
        "  search = arxiv.Search(id_list=re.findall(r'(\\d{4}\\.\\d{5}|\\w+(?:-\\w+)?/\\d{7})', arxiv_id), max_results=1)\n",
        "  results = client.results(search)\n",
        "\n",
        "  for result in results:\n",
        "    return result.title, result.pdf_url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrpPrN3vv6dk"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "desc = []\n",
        "\n",
        "\n",
        "for page in pages_png:\n",
        "  # Getting the base64\n",
        "  base64_image = encode_image(page)\n",
        "\n",
        "  try:\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-2024-08-06\",\n",
        "        response_format = json_response_format,\n",
        "        temperature = 0,\n",
        "        messages= [\n",
        "              {\"role\": \"system\",\"content\":system_instruction_prompt},\n",
        "              {\"role\": \"user\",\"content\": [{\"type\": \"text\", \"text\": \"Extract the content from this research paper image.\"},\n",
        "                                          {\"type\": \"image_url\",\"image_url\": {\"url\":f\"data:image/jpeg;base64,{base64_image}\",\n",
        "                                                                              \"detail\": \"high\"}}\n",
        "                                          ]\n",
        "                  }\n",
        "                  ],\n",
        "      )\n",
        "\n",
        "    if response.choices[0].message.content is None:\n",
        "      continue\n",
        "\n",
        "    result = json.loads(response.choices[0].message.content)\n",
        "\n",
        "    if 'page-001' in page:\n",
        "      # Or You can use the Image path to extract the Arxiv Research paper ID.\n",
        "      research_paper_id = result['research_paper_data'][0]['source']\n",
        "      research_paper_title, research_paper_url = arxiv_extraction(research_paper_id)\n",
        "\n",
        "      for i in range(len(result['research_paper_data'])):\n",
        "        result['research_paper_data'][i]['source'] = research_paper_id\n",
        "        result['research_paper_data'][i]['name'] = research_paper_title +\":\"+ result['research_paper_data'][i]['name']\n",
        "        result['research_paper_data'][i]['url'] = research_paper_url\n",
        "\n",
        "    if 'page-001' not in page:\n",
        "      for i in range(len(result['research_paper_data'])):\n",
        "        result['research_paper_data'][i]['source'] = research_paper_id\n",
        "        result['research_paper_data'][i]['name'] = research_paper_title +\":\"+ result['research_paper_data'][i]['name']\n",
        "        result['research_paper_data'][i]['url'] = research_paper_url\n",
        "\n",
        "    desc.extend(result['research_paper_data'])\n",
        "\n",
        "  except Exception as e:\n",
        "    print(response.choices[0].finish_reason)\n",
        "    print(f\"Skipping {page}... error: {e}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPlZyDWXfJhx",
        "outputId": "dc6529ac-6a3a-41cf-e0df-5180249eacbd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "358"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(desc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TNd7zjLZmHU_",
        "outputId": "1de0af1e-befa-4edc-f4df-ee338e236a2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Content research paper title and Headline : Speculative RAG: Enhancing Retrieval Augmented Generation through Drafting:Speculative RAG: Enhancing Retrieval Augmented Generation through Drafting \n",
            "\n",
            "Content : Zilong Wang¹, Zifeng Wang², Long Le², Huaixiu Steven Zheng³, Swaroop Mishra³, Vincent Perot³, Yuwei Zhang¹, Anush Mattapalli⁴, Ankur Taly⁴, Jingbo Shang¹, Chen-Yu Lee², Tomas Pfister²\n",
            "¹University of California, San Diego ²Google Cloud AI Research ³Google DeepMind ⁴Google Cloud AI\n",
            "\n",
            "Abstract\n",
            "Retrieval augmented generation (RAG) combines the generative abilities of large language models (LLMs) with external knowledge sources to provide more accurate and up-to-date responses. Recent RAG advancements focus on improving retrieval outcomes through iterative LLM refinement or self-critique capabilities acquired through additional instruction tuning of LLMs. In this work, we introduce SPECULATIVE RAG – a framework that leverages a larger generalist LM to efficiently verify multiple RAG drafts produced in parallel by a smaller, distilled specialist LM. Each draft is generated from a distinct subset of retrieved documents, offering diverse perspectives on the evidence while reducing input token counts per draft. This approach enhances comprehension of each subset and mitigates potential position bias over long context. Our method accelerates RAG by delegating drafting to the smaller specialist LM, with the larger generalist LM performing a single verification pass over the drafts. Extensive experiments demonstrate that SPECULATIVE RAG achieves state-of-the-art performance with reduced latency on TriviaQA, MuSiQue, PubHealth, and ARC-Challenge benchmarks. It notably enhances accuracy by up to 12.97% while reducing latency by 51% compared to conventional RAG systems on PubHealth.\n",
            "\n",
            "1 Introduction\n",
            "Large language models (LLMs) have demonstrated remarkable success in question answering tasks (Brown et al., 2020; Achiam et al., 2023; Team et al., 2023). Trained on massive datasets, LLMs leverage their extensive parametric memory to generate seemingly plausible responses to user queries (Kojima et al., 2022; Kamalloo et al., 2023). However, when faced with knowledge-intensive questions demanding up-to-date information or obscure facts (Petroni et al., 2021), LLMs can struggle with factual inaccuracies and produce hallucinated content (Huang et al., 2023; Xu et al., 2024).\n",
            "\n",
            "Retrieval Augmented Generation (RAG) has emerged as a promising solution to mitigate these issues. By incorporating information retrieved from an external database into the context (Gao et al., 2023b), RAG effectively reduces factual errors in knowledge-intensive tasks. This approach not only enables easy and efficient access to vast databases but also facilitates timely and accurate knowledge integration. Due to the inherent limitations in the precision of current dense retrievers and the vastness of knowledge required to answer complex questions (Chen et al., 2022), RAG systems typically retrieve multiple documents to ensure the inclusion of all necessary information in the context (Petroni et al., 2021). This practice inevitably increases the length of the input to the LLMs. \n",
            "\n",
            "Source : 2407.08223v1 \n",
            "\n",
            "URL : http://arxiv.org/pdf/2407.08223v1\n"
          ]
        }
      ],
      "source": [
        "print(\"Content research paper title and Headline :\",desc[0]['name'],\"\\n\")\n",
        "print(\"Content :\",desc[0]['content'],\"\\n\")\n",
        "print(\"Source :\",desc[0]['source'],\"\\n\")\n",
        "print(\"URL :\",desc[0]['url'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuKCik2njQlh",
        "outputId": "731cbdb3-c2cd-4d3d-cc9b-4d9e81f76a03"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'name': 'Speculative RAG: Enhancing Retrieval Augmented Generation through Drafting:Illustration of Different RAG Approaches',\n",
              " 'source': '2407.08223v1',\n",
              " 'content': 'Figure 1: Illustration of different RAG approaches. Given a knowledge-intensive query Q and retrieved documents, (a) Standard RAG incorporates all documents into the prompt, increasing input length and slowing inference; (b) Self-Reflective RAG (Asai et al., 2023) requires specialized instruction-tuning of the general-purpose language model (LM) to generate specific tags for self-reflection; (c) Corrective RAG (Yan et al., 2024) employs an external retrieval evaluator to refine document quality, focusing solely on contextual information without enhancing reasoning capabilities; (d) In contrast, our proposed SPECULATIVE RAG leverages a larger generalist LM to efficiently verify multiple RAG drafts produced in parallel by a smaller, specialized LM. Each draft is generated from a distinct subset of retrieved documents, providing diverse perspectives on the evidence while minimizing the number of input tokens per draft.\\n\\nIn this world, we introduce SPECULATIVE RAG, a RAG framework designed to offload computational burden to a smaller, specialist LM that serves as an efficient and robust RAG module for existing generalist LMs. Inspired by Speculative Decoding (Leviathan et al., 2023; Chen et al., 2023a; Xia et al., 2024), which accelerates auto-regressive LM inference by concurrently generating multiple draft tokens with a smaller model and verifying them in parallel with the base model, our approach adapts this concept to RAG.\\n\\nIn SPECULATIVE RAG, we partition retrieved documents into subsets for drafting answer candidates. We cluster the retrieved documents by content similarity and sample one document from each cluster to form a subset, minimizing redundancy and maximizing diversity. These document subsets are then fed to multiple instances of the RAG module, which generate draft answers with corresponding rationales in parallel. This smaller, specialized RAG module, excels at reasoning over retrieved documents and can rapidly produce accurate responses. Subsequently, the generalist LM bypasses the detailed review of potentially repetitive documents, focusing instead on validating the drafts.',\n",
              " 'url': 'http://arxiv.org/pdf/2407.08223v1'}"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "desc[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcIr06B_8HG6",
        "outputId": "c519d671-5d8a-4ee9-c561-155cc64ac0de"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['2407.07858v1',\n",
              " '2408.08921v1',\n",
              " '2407.21712v1',\n",
              " '2407.16833v1',\n",
              " '2407.01219v1',\n",
              " '2405.07437v2',\n",
              " '2407.08223v1',\n",
              " '2408.08067v2']"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "source=[]\n",
        "for i in range(len(desc)):\n",
        "  source.append(desc[i]['source'])\n",
        "\n",
        "source=list(set(source))\n",
        "source"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P64zuFRHub8H"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "e3moEdG_EpX7",
        "outputId": "91b1a7aa-65da-47c1-c65d-4e1d3c3e73e5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'tokens': 3031,\n",
              " 'doc_id': '60deb74f-d8b5-47a6-93f2-425887a46e33',\n",
              " 'name': 'Named Entity Recognition in Ecommerce Industry  Custom model [Github Repo]  03/07/24',\n",
              " 'url': 'https://towardsai.net/p/machine-learning/named-entity-recognition-in-ecommerce-industry-custom-model-github-repo-03-07-24',\n",
              " 'source': 'tai_blog',\n",
              " 'content': \"Github Repo: https://github.com/vaibhawkhemka/ML-Umbrella/tree/main/NLP/Product-Categorization   From e-commerce to Customer support  all businesses require some kind of NER model to process huge amounts of texts from users.   To automate this whole  one requires NER models to extract relevant and important entities from text.   Final Result/OutputInput text = EL D68 (Green  32 GB) 3 GB RAM [3 GB RAM U+007C 32 GB ROM U+007C Expandable Upto 128 GB  15.46 cm (6.088 inch) Display  13MP Rear Camera U+007C 8MP Front Camera  4000 mAh Battery  Quad-Core Processor]   Output =   Green ->>>> COLOR 32 GB ->>>> STORAGE 3 GB RAM ->>>> RAM 3 GB RAM ->>>> RAM 32 GB ROM ->>>> STORAGE Expandable Upto 128 GB ->>>> EXPANDABLE_STORAGE 15.46 cm (6.088 inch) ->>>> SCREEN_SIZE 13MP Rear Camera ->>>> BACK_CAMERA 8MP Front Camera ->>>> FRONT_CAMERA 4000 mAh Battery ->>>> BATTERY_CAPACITY Quad-Core Processor ->>>> PROCESSOR_CORE   Data PreparationA tool for creating this dataset (https://github.com/tecoholic/ner-annotator)    Snapshot for the dataset for Mobile phone product description on Amazon:   A single record of the Data:   Converting into proper Spacy span format:The proper format that Spacy Ner model understands   import jsonlines  json file_path = Training Data/Mobile/Mobile_training.jsonl laptop_classes = [RAM STORAGE BATTERY CAPACITY PROCESSOR_TYPE SCREEN_SIZE REFRESH_RATE SCREEN_TYPE BACK_CAMERA FRONT_CAMERA] with jsonlines.open(file_path) as reader: output_json = {classes: laptop_classes  annotations: []} # Iterate over each line (JSON object) for obj in reader: processed_obj = [obj[text] {entities:obj[label]}] output_json[annotations].append(processed_obj) # Save the output JSON to a new file with open('Training Data/Mobile/Mobile_annotations.json'  'w') as f: json.dump(output_json  f  indent=None)Above is the code for converting into proper data format. Check out jupyter notebook: NER_model_Mobile.ipynb   Final pandas dataframe from processed data:   Splitting the dataset  10% test### Split the data from sklearn.model_selection import train_test_split train  test = train_test_split(df  test_size=0.1) train.head()Create spacy DocBin objects from annotated data to train Spacy NER model:import spacy from spacy.tokens import DocBin from tqdm import tqdm # Define a function to create spaCy DocBin objects from the annotated data def get_spacy_doc(data): # Create a blank spaCy pipeline nlp = spacy.blank('en') db = DocBin() # Initialize a counter for None spans none_spans = 0 spans = 0 for index  row in data.iterrows(): # Get the text and annotations text = row[Description] annotations = row[Annotations] # Check if the text is not empty if not text: continue # Process the text and annotations doc = nlp(text) if doc is None: print(fFailed to process text: {text}) continue ents = [] for start  end  label in annotations: if start < 0 or end < 0: print(fInvalid annotation: {start}  {end}  {label}) continue #print(text) span = doc.char_span(start  end  label=label) if span is None: print(fFailed to create span for annotation: {start}  {end}  {label}) none_spans += 1 continue else: spans+=1 ents.append(span) doc.ents = ents #Add the processed document to the DocBin db.add(doc) print(fNumber of None spans: {none_spans}) print(fNumber of spans: {spans}) return dbModellingArchitecture:The basic architecture for all spacy models:   Reference: https://explosion.ai/blog/deep-learning-formula-nlp   [Embed]HashEmbed  Sub-word features than character based richer representation and arbitrary sized vocabulary  Can use Word2vec/Glove etc   [Encode]  Context-independent to context-dependent using LSTM or CNN.   [Attend]  Attention mechanism by Key  Value pair  and context vectors   [Predict]  MLP   Tok2vec model [example]:   https://github.com/explosion/spaCy/blob/master/spacy/ml/models/tok2vec.py (Built using thinc framework)   NER Model  Transition-Based:   State(all three stack  buffer  and output) and Action   Structure Prediction.   The above shows how the transition-based approach works with stack  buffer  output  and Transition/action.   Reference: https://www.microsoft.com/en-us/research/video/transition-based-natural-language-processing/   The above shows How stacked LSTM works for encoding for all states and actions.   The final Prediction from MLP is the Multiclassification task with labels as SHIFT  OUT  and REDUCE   Spacy model layer and Config Mapping:   Example of a tok2vec config:   Model in thinc framework:   Respective config for the model:   Thinc deep learning framework is used as a backend to build spacy models instead of pytorch or TensorFlow.   Difference between normal pytorch and spacy models. => Spacy(easy  reliable and productionable)   The user can define and create this model using a configuration file for any task: NER  Tok2Vec  Tagger  Dependency Parser  Sentiment etc   One can also create thinc models and wrap around pytorch and TensorFlow. I will build it next blog.   NER Config file created here:   Reference: https://spacy.io/usage/training   config_ner.cfg :   [paths] train = null dev = null vectors = en_core_web_lg init_tok2vec = null [system] gpu_allocator = null seed = 0 [nlp] lang = en pipeline = [tok2vec ner] batch_size = 1000 disabled = [] before_creation = null after_creation = null after_pipeline_creation = null tokenizer = {@tokenizers:spacy.Tokenizer.v1} vectors = {@vectors:spacy.Vectors.v1} [components] [components.ner] factory = ner incorrect_spans_key = null moves = null scorer = {@scorers:spacy.ner_scorer.v1} update_with_oracle_cut_size = 100 [components.ner.model] @architectures = spacy.TransitionBasedParser.v2 state_type = ner extra_state_tokens = false hidden_width = 64 maxout_pieces = 2 use_upper = true nO = null [components.ner.model.tok2vec] @architectures = spacy.Tok2VecListener.v1 width = ${components.tok2vec.model.encode.width} upstream = * [components.tok2vec] factory = tok2vec [components.tok2vec.model] @architectures = spacy.Tok2Vec.v2 [components.tok2vec.model.embed] @architectures = spacy.MultiHashEmbed.v2 width = ${components.tok2vec.model.encode.width} attrs = [NORM PREFIX SUFFIX SHAPE] rows = [5000 1000 2500 2500] include_static_vectors = true [components.tok2vec.model.encode] @architectures = spacy.MaxoutWindowEncoder.v2 width = 256 depth = 8 window_size = 1 maxout_pieces = 3 [corpora] [corpora.dev] @readers = spacy.Corpus.v1 path = ${paths.dev} max_length = 0 gold_preproc = false limit = 0 augmenter = null [corpora.train] @readers = spacy.Corpus.v1 path = ${paths.train} max_length = 0 gold_preproc = false limit = 0 augmenter = null [training] dev_corpus = corpora.dev train_corpus = corpora.train seed = ${system.seed} gpu_allocator = ${system.gpu_allocator} dropout = 0.1 accumulate_gradient = 1 patience = 1600 max_epochs = 0 max_steps = 20000 eval_frequency = 200 frozen_components = [] annotating_components = [] before_to_disk = null before_update = null [training.batcher] @batchers = spacy.batch_by_words.v1 discard_oversize = false tolerance = 0.2 get_length = null [training.batcher.size] @schedules = compounding.v1 start = 100 stop = 1000 compound = 1.001 t = 0.0 [training.logger] @loggers = spacy.ConsoleLogger.v1 progress_bar = false [training.optimizer] @optimizers = Adam.v1 beta1 = 0.9 beta2 = 0.999 L2_is_weight_decay = true L2 = 0.01 grad_clip = 1.0 use_averages = false eps = 0.00000001 learn_rate = 0.001 [training.score_weights] ents_f = 1.0 ents_p = 0.0 ents_r = 0.0 ents_per_type = null [pretraining] [initialize] vectors = ${paths.vectors} init_tok2vec = ${paths.init_tok2vec} vocab_data = null lookups = null before_init = null after_init = null [initialize.components] [initialize.tokenizer]Output and Evaluation:Evaluation is done based on ENTS_P(Precision)  ENTS_R(Recall) and ENTS_F (F-Score).   After the 15th epoch Final ENTS_F is 57.64  which can be improved by providing more data for this case.   Intuition for Evaluation:We evaluate the NER model based on Span-Identification and Span-Prediction.   Span-Identification:   https://cees-roele.medium.com/custom-evaluation-of-spans-in-spacy-f1f2e7a99ad8   As discussed  NER is a multiclass Classification problem with SHIFT  OUT  and REDUCE as output. But we evaluate our models only based on REDUCE.   The above picture shows how Precision  Recall  and F-Score are calculated.   The code used for evaluating PRF (Precision-Recall-Fscore) by spacy:   def get_ner_prf(examples: Iterable[Example]  **kwargs) -> Dict[str  Any]: Compute micro-PRF and per-entity PRF scores for a sequence of examples. score_per_type = defaultdict(PRFScore) for eg in examples: if not eg.y.has_annotation(ENT_IOB): continue golds = {(e.label_  e.start  e.end) for e in eg.y.ents} align_x2y = eg.alignment.x2y for pred_ent in eg.x.ents: if pred_ent.label_ not in score_per_type: score_per_type[pred_ent.label_] = PRFScore() indices = align_x2y[pred_ent.start : pred_ent.end] if len(indices): g_span = eg.y[indices[0] : indices[-1] + 1] # Check we aren't missing annotation on this span. If so  # our prediction is neither right nor wrong  we just # ignore it. if all(token.ent_iob != 0 for token in g_span): key = (pred_ent.label_  indices[0]  indices[-1] + 1) if key in golds: score_per_type[pred_ent.label_].tp += 1 golds.remove(key) else: score_per_type[pred_ent.label_].fp += 1 for label  start  end in golds: score_per_type[label].fn += 1 totals = PRFScore() for prf in score_per_type.values(): totals += prf if len(totals) > 0: return { ents_p: totals.precision  ents_r: totals.recall  ents_f: totals.fscore  ents_per_type: {k: v.to_dict() for k  v in score_per_type.items()}  } else: return { ents_p: None  ents_r: None  ents_f: None  ents_per_type: None  }Reference: https://github.com/explosion/spaCy/blob/master/spacy/scorer.py#L760   Span Prediction :   There are 9 different entires like [RAM  STORAGE  BATTERY CAPACITY  PROCESSOR_TYPE  SCREEN_SIZE  REFRESH_RATE  SCREEN_TYPE  BACK_CAMERA  FRONT_CAMERA] to predict for REDUCE class.   It uses categorical crossentropy loss function to optimize NER models (More details in later blogs)   Testing and Final Results:Input text = EL D68 (Green  32 GB) 3 GB RAM [3 GB RAM U+007C 32 GB ROM U+007C Expandable Upto 128 GB  15.46 cm (6.088 inch) Display  13MP Rear Camera U+007C 8MP Front Camera  4000 mAh Battery  Quad-Core Processor]   Output =   Green ->>>> COLOR 32 GB ->>>> STORAGE 3 GB RAM ->>>> RAM 3 GB RAM ->>>> RAM 32 GB ROM ->>>> STORAGE Expandable Upto 128 GB ->>>> EXPANDABLE_STORAGE 15.46 cm (6.088 inch) ->>>> SCREEN_SIZE 13MP Rear Camera ->>>> BACK_CAMERA 8MP Front Camera ->>>> FRONT_CAMERA 4000 mAh Battery ->>>> BATTERY_CAPACITY Quad-Core Processor ->>>> PROCESSOR_CORE   Github Link: https://github.com/vaibhawkhemka/ML-Umbrella/tree/main/NLP/Product-Categorization   Thanks for reading the blog.   If you have any questions  hit me up on my LinkedIn: https://www.linkedin.com/in/vaibhaw-khemka-a92156176/   References for modeling:   https://explosion.ai/blog/deep-learning-formula-nlp => Embed  Encode  Attend and Predict => Position is imp in sequence in text.   https://support.prodi.gy/t/spacy-ner-models-architecture-details/4336   https://github.com/explosion/spaCy/blob/master/spacy/ml/models/tok2vec.py   https://spacy.io/usage/layers-architectures   https://spacy.io/api/architectures#CharacterEmbed   Understanding span:   https://spacy.io/api/span\"}"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# To incldue Generated response with collected content\n",
        "import json\n",
        "with open(\"/content/ai_tutor_500.jsonl\", \"r\") as file:\n",
        "    result = [json.loads(line) for line in file]\n",
        "\n",
        "result[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHWri74kx-22",
        "outputId": "6cf9d7aa-2dff-4250-acc2-79d67866da5f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3031"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result[1]['tokens']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMMsh0DsmTJ4"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "import uuid\n",
        "\n",
        "def token_encoding_and_doc_id(content):\n",
        "  encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "  tokens = encoding.encode(content)\n",
        "\n",
        "  num_tokens = len(tokens)\n",
        "  doc_id = str(uuid.uuid5(uuid.NAMESPACE_DNS, content))\n",
        "\n",
        "  return num_tokens, doc_id\n",
        "\n",
        "for i in range(len(desc)):\n",
        "  desc[i]['tokens'], desc[i]['doc_id'] = token_encoding_and_doc_id(desc[i]['content'])\n",
        "\n",
        "for i in range(len(result)):\n",
        "  result[i]['tokens'], result[i]['doc_id'] = token_encoding_and_doc_id(result[i]['content'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScadYbE-xnl8"
      },
      "outputs": [],
      "source": [
        "ai_tutor_knowledge = result + desc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cx_w2XckxpFo",
        "outputId": "479f4b22-f639-46e8-ebb6-68f943d7866a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'name': 'Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach:References',\n",
              " 'source': 'Arxiv: 2407.16833',\n",
              " 'content': 'Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Dmitry Sorokin, Artyom Sorsokin, and Mikhail Burtsev. 2024. In search of needles in a 10m haystack: Recurrent memory finds what lms miss. arXiv preprint arXiv:2402.10790.',\n",
              " 'url': 'http://arxiv.org/pdf/2407.16833v1',\n",
              " 'tokens': 74,\n",
              " 'doc_id': '55194ca6-3fcc-50ba-914f-e07577bc9973'}"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ai_tutor_knowledge[850]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4U1ct2h4a2H"
      },
      "outputs": [],
      "source": [
        "ai_tutor_knowledge = [d for d in ai_tutor_knowledge if d['tokens'] > 100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvEaPAe9Lir2",
        "outputId": "be25dfed-d5c8-49e5-d947-834174534070"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JSONL file saved successfully!\n"
          ]
        }
      ],
      "source": [
        "with open('/content/ai_tutor_knowledge.jsonl', 'w') as f:\n",
        "    for item in ai_tutor_knowledge:\n",
        "        f.write(json.dumps(item) + '\\n')\n",
        "\n",
        "print(\"JSONL file saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "B4RWqf0bdHnq",
        "outputId": "a152c05d-9738-492e-9c2d-18c4132e1eb8"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_94c8d77b-6bf8-43ca-b85d-26190f911f4a\", \"ai_tutor_knowledge.jsonl\", 7166796)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download('/content/ai_tutor_knowledge.jsonl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFdK63RdjaHr"
      },
      "source": [
        "------------"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}