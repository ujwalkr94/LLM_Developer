{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ujwalkr94/LLM_Developer/blob/main/notebooks/Crawl_a_Website.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5TdQCZ6Jt-1"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/towardsai/ai-tutor-rag-system/blob/main/notebooks/Crawl_a_Website.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CW8ux1RSdem",
        "outputId": "62fb2afe-ae14-4ea0-be5b-bdd69fbba557"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/7.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/7.4 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m6.6/7.4 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m92.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m455.6/455.6 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.9/104.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.0/241.0 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q llama-index==0.12.11 llama-index-llms-openai==0.3.13 openai==1.59.8 google-generativeai==0.8.3 newspaper3k==0.2.8 lxml_html_clean==0.4.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxDPsVXSAj6_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Set the following API Keys in the Python environment. Will be used later.\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"[OPENAI_API_KEY]\"\n",
        "USESCRAPER_API_KEY = \"[USESCRAPER_API_KEY]\"\n",
        "\n",
        "# from google.colab import userdata\n",
        "# os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY1')\n",
        "# os.environ[\"USESCRAPER_API_KEY\"] = userdata.get('USESCRAPER_API_KEY')\n",
        "\n",
        "# USESCRAPER_API_KEY = userdata.get('USESCRAPER_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSc7-1mljmrp"
      },
      "source": [
        "There are two primary methods for extracting webpage content. The first method involves having a list of URLs; one can iterate through this list to retrieve the content of each page. The second method, web crawling, requires using a script or service to extract page URLs from a sitemap or manually following links on the page to access all the content. Initially, we will explore web scraping techniques before discussing how to use a service like usescraper.com to perform web crawling.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3r2tYHgeIK9"
      },
      "source": [
        "# 1. Scraping using `newspaper` Library\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "it43ZQf8jatw"
      },
      "source": [
        "## Define URLs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x74PqfQ7eIzD"
      },
      "outputs": [],
      "source": [
        "urls = [\n",
        "    \"https://docs.llamaindex.ai/en/stable/understanding\",\n",
        "    \"https://docs.llamaindex.ai/en/stable/understanding/using_llms/using_llms/\",\n",
        "    \"https://docs.llamaindex.ai/en/stable/understanding/indexing/indexing/\",\n",
        "    \"https://docs.llamaindex.ai/en/stable/understanding/querying/querying/\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgxfpfSsjcMC"
      },
      "source": [
        "## Get Page Contents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6Xs1OhUfVQV"
      },
      "outputs": [],
      "source": [
        "import newspaper\n",
        "\n",
        "pages_content = []\n",
        "\n",
        "# Retrieve the Content\n",
        "for url in urls:\n",
        "    try:\n",
        "        article = newspaper.Article(url)\n",
        "        article.download()\n",
        "        article.parse()\n",
        "        if len(article.text) > 0:\n",
        "            pages_content.append(\n",
        "                {\"url\": url, \"title\": article.title, \"text\": article.text}\n",
        "            )\n",
        "    except:\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cNdJNi2g1ly",
        "outputId": "9d153bb1-0633-456a-9ca3-4c97a3846c98"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'url': 'https://docs.llamaindex.ai/en/stable/understanding',\n",
              " 'title': 'Building an LLM Application',\n",
              " 'text': \"Building an LLM application#\\n\\nWelcome to the beginning of Understanding LlamaIndex. This is a series of short, bite-sized tutorials on every stage of building an LLM application to get you acquainted with how to use LlamaIndex before diving into more advanced and subtle strategies. If you're an experienced programmer new to LlamaIndex, this is the place to start.\\n\\nKey steps in building an LLM application#\\n\\nTip If you've already read our high-level concepts page you'll recognize several of these steps.\\n\\nThis tutorial has three main parts: Building a RAG pipeline, Building an agent, and Building Workflows, with some smaller sections before and after. Here's what to expect:\\n\\nUsing LLMs : hit the ground running by getting started working with LLMs. We'll show you how to use any of our dozens of supported LLMs, whether via remote API calls or running locally on your machine.\\n\\nBuilding a RAG pipeline : Retrieval-Augmented Generation (RAG) is a key technique for getting your data into an LLM, and a component of more sophisticated agentic systems. We'll show you how to build a full-featured RAG pipeline that can answer questions about your data. This includes: Loading & Ingestion : Getting your data from wherever it lives, whether that's unstructured text, PDFs, databases, or APIs to other applications. LlamaIndex has hundreds of connectors to every data source over at LlamaHub. Indexing and Embedding : Once you've got your data there are an infinite number of ways to structure access to that data to ensure your applications is always working with the most relevant data. LlamaIndex has a huge number of these strategies built-in and can help you select the best ones. Storing : You will probably find it more efficient to store your data in indexed form, or pre-processed summaries provided by an LLM, often in a specialized database known as a Vector Store (see below). You can also store your indexes, metadata and more. Querying : Every indexing strategy has a corresponding querying strategy and there are lots of ways to improve the relevance, speed and accuracy of what you retrieve and what the LLM does with it before returning it to you, including turning it into structured responses such as an API.\\n\\nBuilding an agent : agents are LLM-powered knowledge workers that can interact with the world via a set of tools. Those tools can be RAG engines such as you learned how to build in the previous section, or any arbitrary code. This tutorial includes: Building a basic agent : We show you how to build a simple agent that can interact with the world via a set of tools. Using local models with agents : Agents can be built to use local models, which can be important for performance or privacy reasons. Adding RAG to an agent : The RAG pipelines you built in the previous tutorial can be used as a tool by an agent, giving your agent powerful information-retrieval capabilities. Adding other tools : Let's add more sophisticated tools to your agent, such as API integrations.\\n\\nBuilding Workflows : Workflows are a low-level, event-driven abstraction for building agentic applications. They're the base layer you should be using to build any custom, advanced RAG/agent system. You can use the pre-built abstractions you learned above, or build completely agentic applications from scratch. Get started here.\\n\\nPutting it all together : whether you are building question & answering, chatbots, an API, or an autonomous agent, we show you how to get your application into production.\\n\\nTracing and debugging : also called observability , it's especially important with LLM applications to be able to look into the inner workings of what's going on to help you debug problems and spot places to improve.\\n\\nEvaluating: every strategy has pros and cons and a key part of building, shipping and evolving your application is evaluating whether your change has improved your application in terms of accuracy, performance, clarity, cost and more. Reliably evaluating your changes is a crucial part of LLM application development.\\n\\nReady to dive in? Head to using LLMs.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "pages_content[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WleP60A3gkQM",
        "outputId": "c707eb4b-a604-4fd4-ebdf-20dfb654ed0c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "len(pages_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5mCiRfGjfNx"
      },
      "source": [
        "## Convert to Document\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOJ3K-CBfVDR"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.schema import Document\n",
        "\n",
        "# Convert the chunks to Document objects so the LlamaIndex framework can process them.\n",
        "documents = [\n",
        "    Document(text=row[\"text\"], metadata={\"title\": row[\"title\"], \"url\": row[\"url\"]})\n",
        "    for row in pages_content\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkjEyEmkJevT"
      },
      "source": [
        "# 2. Submit the Crawler Job\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYpchBo5-brp",
        "outputId": "134b2182-1577-4dc2-c548-e3c2f2fc02fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'org': '2492', 'id': '7YDDVX24F9WD1JMC1EE83PT88Q', 'urls': ['https://docs.llamaindex.ai/en/stable/understanding/'], 'exclude_globs': [], 'exclude_elements': 'nav, header, footer, script, style, noscript, svg, [role=\"alert\"], [role=\"banner\"], [role=\"dialog\"], [role=\"alertdialog\"], [role=\"region\"][aria-label*=\"skip\" i], [aria-modal=\"true\"]', 'output_format': 'markdown', 'output_expiry': 604800, 'min_length': 50, 'page_limit': 3, 'force_crawling_mode': 'link', 'block_resources': True, 'include_linked_files': False, 'createdAt': 1737451564566, 'status': 'starting', 'use_browser': True, 'sitemapPageCount': 0, 'notices': []}\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "urls_to_crawl = [\n",
        "    \"https://docs.llamaindex.ai/en/stable/understanding/\", # add your URLs here, e.g. \"https://docs.llamaindex.ai/en/stable/understanding/\"\n",
        "]\n",
        "\n",
        "payload = {\n",
        "    \"urls\": urls_to_crawl,  # list of urls to crawl\n",
        "    \"output_format\": \"markdown\",  # text, html, markdown\n",
        "    \"output_expiry\": 604800,  # Automatically delete after X seconds\n",
        "    \"min_length\": 50,  # Skip pages with less than X characters\n",
        "    \"page_limit\": 3,  # Maximum number of pages to crawl\n",
        "    \"force_crawling_mode\": \"link\",  # \"link\" follows links in the page reccursively, or \"sitemap\" to find pages from website's sitemap\n",
        "    \"block_resources\": True,  # skip loading images, stylesheets, or scripts\n",
        "    \"include_linked_files\": False,  # include files (PDF, text, ...) in output\n",
        "}\n",
        "headers = {\n",
        "    \"Authorization\": \"Bearer \" + USESCRAPER_API_KEY,\n",
        "    \"Content-Type\": \"application/json\",\n",
        "}\n",
        "\n",
        "response = requests.request(\n",
        "    \"POST\", \"https://api.usescraper.com/crawler/jobs\", json=payload, headers=headers\n",
        ")\n",
        "\n",
        "response = json.loads(response.text)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nx_4MjHxJgxh"
      },
      "source": [
        "## Get the Status\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLJ0BUR8c1a8",
        "outputId": "b5d43bbc-cbfd-4477-ae41-3f0a3fa9daad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "succeeded\n",
            "{'scraped': 3, 'discarded': 0, 'failed': 0}\n"
          ]
        }
      ],
      "source": [
        "url = \"https://api.usescraper.com/crawler/jobs/{}\".format(response[\"id\"])\n",
        "\n",
        "status_res = requests.request(\"GET\", url, headers=headers)\n",
        "\n",
        "status_res = json.loads(status_res.text)\n",
        "\n",
        "print(status_res[\"status\"])\n",
        "print(status_res[\"progress\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHcRJIDsJh2i"
      },
      "source": [
        "## Get the Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4dUn4cmGGab",
        "outputId": "7815d163-62c1-427e-d9e6-31b0da900d8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'data': [{'meta': {'url': 'https://docs.llamaindex.ai/en/stable/understanding/agent/multi_agents/', 'fetchedUrl': 'https://docs.llamaindex.ai/en/stable/understanding/agent/multi_agents/', 'fetchedUrlStatusCode': 200, 'meta': {'title': 'Multi-agent workflows - LlamaIndex'}}, 'text': ' \\n[ Skip to content ](https://docs.llamaindex.ai/en/stable/understanding/agent/multi_agents/#multi-agent-workflows)\\n#Multi-Agent Workflows[#](https://docs.llamaindex.ai/en/stable/understanding/agent/multi_agents/#multi-agent-workflows)\\nThe AgentWorkflow uses Workflow Agents to allow you to create a system of one or more agents that can collaborate and hand off tasks to each other based on their specialized capabilities. This enables building complex agent systems where different agents handle different aspects of a task.\\n\\nTip\\n\\nThe AgentWorkflow class is built on top of LlamaIndex Workflows. For more information on how workflows work, check out the [detailed guide](https://docs.llamaindex.ai/en/stable/module_guides/workflow/) or [introductory tutorial](https://docs.llamaindex.ai/en/stable/understanding/workflows/).\\n\\n##Quick Start[#](https://docs.llamaindex.ai/en/stable/understanding/agent/multi_agents/#quick-start)\\nHere\\'s a simple example of setting up a multi-agent workflow with a calculator agent and a retriever agent:\\n\\nfrom llama_index.core.agent.workflow import (\\n    AgentWorkflow,\\n    FunctionAgent,\\n    ReActAgent,\\n)\\nfrom llama_index.core.tools import FunctionTool\\n\\n\\n# Define some tools\\ndef add(a: int, b: int) -> int:\\n    \"\"\"Add two numbers.\"\"\"\\n    return a + b\\n\\n\\ndef subtract(a: int, b: int) -> int:\\n    \"\"\"Subtract two numbers.\"\"\"\\n    return a - b\\n\\n\\n# Create agent configs\\n# NOTE: we can use FunctionAgent or ReActAgent here.\\n# FunctionAgent works for LLMs with a function calling API.\\n# ReActAgent works for any LLM.\\ncalculator_agent = FunctionAgent(\\n    name=\"calculator\",\\n    description=\"Performs basic arithmetic operations\",\\n    system_prompt=\"You are a calculator assistant.\",\\n    tools=[\\n        FunctionTool.from_defaults(fn=add),\\n        FunctionTool.from_defaults(fn=subtract),\\n    ],\\n    llm=OpenAI(model=\"gpt-4\"),\\n)\\n\\nretriever_agent = FunctionAgent(\\n    name=\"retriever\",\\n    description=\"Manages data retrieval\",\\n    system_prompt=\"You are a retrieval assistant.\",\\n    llm=OpenAI(model=\"gpt-4\"),\\n)\\n\\n# Create and run the workflow\\nworkflow = AgentWorkflow(\\n    agents=[calculator_agent, retriever_agent], root_agent=\"calculator\"\\n)\\n\\n# Run the system\\nresponse = await workflow.run(user_msg=\"Can you add 5 and 3?\")\\n\\n#  Or stream the events\\nhandler = workflow.run(user_msg=\"Can you add 5 and 3?\")\\nasync for event in handler.stream_events():\\n    if hasattr(event, \"delta\"):\\n        print(event.delta, end=\"\", flush=True)\\n##How It Works[#](https://docs.llamaindex.ai/en/stable/understanding/agent/multi_agents/#how-it-works)\\nThe AgentWorkflow manages a collection of agents, each with their own specialized capabilities. One agent must be designated as the root agent in the AgentWorkflow constructor.\\n\\nWhen a user message comes in, it\\'s first routed to the root agent. Each agent can then:\\n\\n- Handle the request directly using their tools\\n- Hand off to another agent better suited for the task\\n- Return a response to the user\\n##Configuration Options[#](https://docs.llamaindex.ai/en/stable/understanding/agent/multi_agents/#configuration-options)\\n###Agent Workflow Config[#](https://docs.llamaindex.ai/en/stable/understanding/agent/multi_agents/#agent-workflow-config)\\nEach agent holds a certain set of configuration options. Whether you use FunctionAgent or ReActAgent, the core options are the same.\\n\\nFunctionAgent(\\n    # Unique name for the agent (str)\\n    name=\"name\",\\n    # Description of agent\\'s capabilities (str)\\n    description=\"description\",\\n    # System prompt for the agent (str)\\n    system_prompt=\"system_prompt\",\\n    # Tools available to this agent (List[BaseTool])\\n    tools=[...],\\n    # LLM to use for this agent. (BaseLLM)\\n    llm=OpenAI(model=\"gpt-4\"),\\n    # List of agents this one can hand off to. Defaults to all agents. (List[str])\\n    can_handoff_to=[...],\\n)\\n###Workflow Options[#](https://docs.llamaindex.ai/en/stable/understanding/agent/multi_agents/#workflow-options)\\nThe AgentWorkflow constructor accepts:\\n\\nAgentWorkflow(\\n    # List of agent configs. (List[BaseWorkflowAgent])\\n    agents=[...],\\n    # Root agent name. (str)\\n    root_agent=\"root_agent\",\\n    # Initial state dict. (Optional[dict])\\n    initial_state=None,\\n    # Custom prompt for handoffs. Should contain the `agent_info` string variable. (Optional[str])\\n    handoff_prompt=None,\\n    # Custom prompt for state. Should contain the `state` and `msg` string variables. (Optional[str])\\n    state_prompt=None,\\n    # Timeout for the workflow, in seconds. (Optional[float])\\n    timeout=None,\\n)\\n###State Management[#](https://docs.llamaindex.ai/en/stable/understanding/agent/multi_agents/#state-management)\\n####Initial Global State[#](https://docs.llamaindex.ai/en/stable/understanding/agent/multi_agents/#initial-global-state)\\nYou can provide an initial state dict that will be available to all agents:\\n\\nworkflow = AgentWorkflow(\\n    agents=[...],\\n    root_agent=\"root_agent\",\\n    initial_state={\"counter\": 0},\\n    state_prompt=\"Current state: {state}. User message: {msg}\",\\n)\\nThe state is stored in the state key of the workflow context. It will be injected into the state_prompt which augments each new user message.\\n\\nThe state can also be modified by tools by accessing the workflow context directly in the tool body.\\n\\n####Persisting State Between Runs[#](https://docs.llamaindex.ai/en/stable/understanding/agent/multi_agents/#persisting-state-between-runs)\\nIn order to persist state between runs, you can pass in the context from the previous run:\\n\\nworkflow = AgentWorkflow(...)\\n\\n# Run the workflow\\nhandler = workflow.run(user_msg=\"Can you add 5 and 3?\")\\nresponse = await handler\\n\\n# Pass in the context from the previous run\\nhandler = workflow.run(ctx=handler.ctx, user_msg=\"Can you add 5 and 3?\")\\nresponse = await handler\\n####Serializing Context / State[#](https://docs.llamaindex.ai/en/stable/understanding/agent/multi_agents/#serializing-context-state)\\nAs with normal workflows, the context is serializable:\\n\\nfrom llama_index.core.workflow import (\\n    Context,\\n    JsonSerializer,\\n    JsonPickleSerializer,\\n)\\n\\n# the default serializer is JsonSerializer for safety\\nctx_dict = handler.ctx.to_dict(serializer=JsonSerializer())\\n\\n# then you can rehydrate the context\\nctx = Context.from_dict(ctx_dict, serializer=JsonSerializer())\\n##Streaming Events[#](https://docs.llamaindex.ai/en/stable/understanding/agent/multi_agents/#streaming-events)\\nThe workflow emits various events during execution that you can stream:\\n\\nasync for event in workflow.run(...).stream_events():\\n    if isinstance(event, AgentInput):\\n        print(event.input)\\n        print(event.current_agent_name)\\n    elif isinstance(event, AgentStream):\\n        # Agent thinking/tool calling response stream\\n        print(event.delta)\\n        print(event.current_agent_name)\\n    elif isinstance(event, AgentOutput):\\n        print(event.response)\\n        print(event.tool_calls)\\n        print(event.raw)\\n        print(event.current_agent_name)\\n    elif isinstance(event, ToolCall):\\n        # Tool being called\\n        print(event.tool_name)\\n        print(event.tool_kwargs)\\n    elif isinstance(event, ToolCallResult):\\n        # Result of tool call\\n        print(event.tool_output)\\n##Accessing Context in Tools[#](https://docs.llamaindex.ai/en/stable/understanding/agent/multi_agents/#accessing-context-in-tools)\\nThe FunctionTool allows tools to access the workflow context if the function has a Context type hint as the first parameter:\\n\\nfrom llama_index.core.tools import FunctionTool\\n\\n\\nasync def get_counter(ctx: Context) -> int:\\n    \"\"\"Get the current counter value.\"\"\"\\n    return await ctx.get(\"counter\", default=0)\\n\\n\\ncounter_tool = FunctionToolWithContext.from_defaults(\\n    async_fn=get_counter, description=\"Get the current counter value\"\\n)\\nTip\\n\\nThe FunctionTool requires the ctx parameter to be passed in explicitly when calling the tool. AgentWorkflow will automatically pass in the context for you.\\n\\n##Human in the Loop[#](https://docs.llamaindex.ai/en/stable/understanding/agent/multi_agents/#human-in-the-loop)\\nUsing the context, you can implement a human in the loop pattern in your tools:\\n\\nfrom llama_index.core.workflow import InputRequiredEvent, HumanResponseEvent\\n\\n\\nasync def ask_for_confirmation(ctx: Context) -> bool:\\n    \"\"\"Ask the user for confirmation.\"\"\"\\n    ctx.write_event_to_stream(\\n        InputRequiredEvent(prefix=\"Please confirm\", confirmation_id=\"1234\")\\n    )\\n\\n    result = await ctx.wait_for_event(\\n        HumanResponseEvent, requirements={\"confirmation_id\": \"1234\"}\\n    )\\n    return result.confirmation\\nWhen this function is called (i.e, when an agent calls this tool), it will block the workflow execution until the user sends the required confirmation event.\\n\\nhandler = workflow.run(user_msg=\"Can you add 5 and 3?\")\\n\\nasync for event in handler.stream_events():\\n    if isinstance(event, InputRequiredEvent):\\n        print(event.confirmation_id)\\n        handler.ctx.send_event(\\n            HumanResponseEvent(response=\"True\", confirmation_id=\"1234\")\\n        )\\n    ...\\n##A Detailed Look at the Workflow[#](https://docs.llamaindex.ai/en/stable/understanding/agent/multi_agents/#a-detailed-look-at-the-workflow)\\nNow that we\\'ve covered the basics, let\\'s take a look at how the workflow operates in more detail using an end-to-end example. In this example, assume we have an AgentWorkflow with two agents: generate and review. In this workflow, generate is the root agent, and responsible for generating content. The review agent is responsible for reviewing the generated content.\\n\\nWhen the user sends in a request, here\\'s the actual sequence of events:\\n\\n- The workflow initializes the context with:\\n- A memory buffer for chat history.\\n- The available agents\\n- The [initial state](https://docs.llamaindex.ai/en/stable/understanding/agent/multi_agents/#initial-global-state) dictionary\\n-\\nThe current agent (initially set to the root agent, generate)\\n\\n-\\nThe user\\'s message is processed:\\n\\n- If [state exists](https://docs.llamaindex.ai/en/stable/understanding/agent/multi_agents/#initial-global-state), it\\'s added to the user\\'s message using the [state prompt](https://docs.llamaindex.ai/en/stable/understanding/agent/multi_agents/#agent-workflow-config)\\n- The message is added to memory\\n-\\nThe chat history is prepared for the current agent\\n\\n-\\nThe current agent is set up:\\n\\n- The agent\\'s tools are gathered (including any retrieved tools)\\n- A special handoff tool is added if the agent can hand off to others\\n- The agent\\'s system prompt is prepended to the chat history\\n-\\nAn AgentInput event is emitted just before the LLM is called\\n\\n-\\nThe agent processes the input:\\n\\n- The agent generates a response and/or makes tool calls. This generates both AgentStream events and an AgentOutput event\\n- If there are no tool calls, the agent finalizes its response and returns it\\n-\\nIf there are tool calls, each tool is executed and the results are processed. This will generate a ToolCall event and a ToolCallResult event for each tool call\\n\\n-\\nAfter tool execution:\\n\\n- If any tool was marked as return_direct=True, its result becomes the final output\\n- If a handoff occurred (via the handoff tool), the workflow switches to the new agent. This will not be added to the chat history in order to maintain the conversation flow.\\n- Otherwise, the updated chat history is sent back to the current agent for another step\\nThis cycle continues until either: - The current agent provides a response without tool calls - A tool marked as return_direct=True is called (except for handoffs) - The workflow times out (if a timeout was configured)\\n\\n##Examples[#](https://docs.llamaindex.ai/en/stable/understanding/agent/multi_agents/#examples)\\nWe have a few notebook examples using the AgentWorkflow class:\\n\\n- [Agent Workflow Overview](https://docs.llamaindex.ai/en/stable/examples/agent/agent_workflow_basic/)\\n- [Multi-Agent Research Report Workflow](https://docs.llamaindex.ai/en/stable/examples/agent/agent_workflow_multi/)\\n\\n'}, {'meta': {'url': 'https://docs.llamaindex.ai/en/stable/understanding/', 'fetchedUrl': 'https://docs.llamaindex.ai/en/stable/understanding/', 'fetchedUrlStatusCode': 200, 'meta': {'title': 'Building an LLM Application - LlamaIndex'}}, 'text': \" \\n[ Skip to content ](https://docs.llamaindex.ai/en/stable/understanding/#building-an-llm-application)\\n#Building an LLM application[#](https://docs.llamaindex.ai/en/stable/understanding/#building-an-llm-application)\\nWelcome to the beginning of Understanding LlamaIndex. This is a series of short, bite-sized tutorials on every stage of building an LLM application to get you acquainted with how to use LlamaIndex before diving into more advanced and subtle strategies. If you're an experienced programmer new to LlamaIndex, this is the place to start.\\n\\n##Key steps in building an LLM application[#](https://docs.llamaindex.ai/en/stable/understanding/#key-steps-in-building-an-llm-application)\\nTip\\n\\nIf you've already read our [high-level concepts](https://docs.llamaindex.ai/en/stable/getting_started/concepts/) page you'll recognize several of these steps.\\n\\nThis tutorial has three main parts: Building a RAG pipeline, Building an agent, and Building Workflows, with some smaller sections before and after. Here's what to expect:\\n\\n-\\n[Using LLMs](https://docs.llamaindex.ai/en/stable/understanding/using_llms/using_llms/): hit the ground running by getting started working with LLMs. We'll show you how to use any of our [dozens of supported LLMs](https://docs.llamaindex.ai/en/stable/module_guides/models/llms/modules/), whether via remote API calls or running locally on your machine.\\n\\n-\\nBuilding a RAG pipeline: Retrieval-Augmented Generation (RAG) is a key technique for getting your data into an LLM, and a component of more sophisticated agentic systems. We'll show you how to build a full-featured RAG pipeline that can answer questions about your data. This includes:\\n\\n-\\n[Loading & Ingestion](https://docs.llamaindex.ai/en/stable/understanding/loading/loading/): Getting your data from wherever it lives, whether that's unstructured text, PDFs, databases, or APIs to other applications. LlamaIndex has hundreds of connectors to every data source over at [LlamaHub](https://llamahub.ai/).\\n\\n-\\n[Indexing and Embedding](https://docs.llamaindex.ai/en/stable/understanding/indexing/indexing/): Once you've got your data there are an infinite number of ways to structure access to that data to ensure your applications is always working with the most relevant data. LlamaIndex has a huge number of these strategies built-in and can help you select the best ones.\\n\\n-\\n[Storing](https://docs.llamaindex.ai/en/stable/understanding/storing/storing/): You will probably find it more efficient to store your data in indexed form, or pre-processed summaries provided by an LLM, often in a specialized database known as a Vector Store (see below). You can also store your indexes, metadata and more.\\n\\n-\\n[Querying](https://docs.llamaindex.ai/en/stable/understanding/querying/querying/): Every indexing strategy has a corresponding querying strategy and there are lots of ways to improve the relevance, speed and accuracy of what you retrieve and what the LLM does with it before returning it to you, including turning it into structured responses such as an API.\\n\\n-\\nBuilding an agent: agents are LLM-powered knowledge workers that can interact with the world via a set of tools. Those tools can be RAG engines such as you learned how to build in the previous section, or any arbitrary code. This tutorial includes:\\n\\n-\\n[Building a basic agent](https://docs.llamaindex.ai/en/stable/understanding/agent/basic_agent.md): We show you how to build a simple agent that can interact with the world via a set of tools.\\n\\n-\\n[Using local models with agents](https://docs.llamaindex.ai/en/stable/understanding/agent/local_models/): Agents can be built to use local models, which can be important for performance or privacy reasons.\\n\\n-\\n[Adding RAG to an agent](https://docs.llamaindex.ai/en/stable/understanding/agent/rag_agent/): The RAG pipelines you built in the previous tutorial can be used as a tool by an agent, giving your agent powerful information-retrieval capabilities.\\n\\n-\\n[Adding other tools](https://docs.llamaindex.ai/en/stable/understanding/agent/tools/): Let's add more sophisticated tools to your agent, such as API integrations.\\n\\n-\\nBuilding Workflows: Workflows are a low-level, event-driven abstraction for building agentic applications. They're the base layer you should be using to build any custom, advanced RAG/agent system. You can use the pre-built abstractions you learned above, or build completely agentic applications from scratch. [Get started here](https://docs.llamaindex.ai/en/stable/understanding/workflows/).\\n\\n-\\n[Putting it all together](https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/): whether you are building question & answering, chatbots, an API, or an autonomous agent, we show you how to get your application into production.\\n\\n-\\n[Tracing and debugging](https://docs.llamaindex.ai/en/stable/understanding/tracing_and_debugging/tracing_and_debugging/): also called observability, it's especially important with LLM applications to be able to look into the inner workings of what's going on to help you debug problems and spot places to improve.\\n\\n-\\n[Evaluating](https://docs.llamaindex.ai/en/stable/understanding/evaluating/evaluating/): every strategy has pros and cons and a key part of building, shipping and evolving your application is evaluating whether your change has improved your application in terms of accuracy, performance, clarity, cost and more. Reliably evaluating your changes is a crucial part of LLM application development.\\n\\n##Let's get started![#](https://docs.llamaindex.ai/en/stable/understanding/#lets-get-started)\\nReady to dive in? Head to [using LLMs](https://docs.llamaindex.ai/en/stable/understanding/using_llms/using_llms/).\\n\\n\\n\"}, {'meta': {'url': 'https://docs.llamaindex.ai/en/stable/understanding/workflows/subclass/', 'fetchedUrl': 'https://docs.llamaindex.ai/en/stable/understanding/workflows/subclass/', 'fetchedUrlStatusCode': 200, 'meta': {'title': 'Subclassing workflows - LlamaIndex'}}, 'text': ' \\n[ Skip to content ](https://docs.llamaindex.ai/en/stable/understanding/workflows/subclass/#subclassing-workflows)\\n#Subclassing workflows[#](https://docs.llamaindex.ai/en/stable/understanding/workflows/subclass/#subclassing-workflows)\\nAnother great feature of workflows is their extensibility. You can take workflows written by others or built-ins from LlamaIndex and extend them to customize them to your needs. We\\'ll look at two ways to do that.\\n\\nThe first is subclassing: workflows are just regular Python classes, which means you can subclass them to add new functionality. For example, let\\'s say you have an agentic workflow that does some processing and then sends an email. You can subclass the workflow to add an extra step to send a text message as well.\\n\\nHere\\'s our base workflow:\\n\\nfrom llama_index.core.workflow import (\\n    StartEvent,\\n    StopEvent,\\n    Workflow,\\n    step,\\n    Event,\\n    Context,\\n)\\n\\n\\nclass Step2Event(Event):\\n    query: str\\n\\n\\nclass Step3Event(Event):\\n    query: str\\n\\n\\nclass MainWorkflow(Workflow):\\n    @step\\n    async def start(self, ev: StartEvent) -> Step2Event:\\n        print(\"Starting up\")\\n        return Step2Event(query=ev.query)\\n\\n    @step\\n    async def step_two(self, ev: Step2Event) -> Step3Event:\\n        print(\"Sending an email\")\\n        return Step3Event(query=ev.query)\\n\\n    @step\\n    async def step_three(self, ev: Step3Event) -> StopEvent:\\n        print(\"Finishing up\")\\n        return StopEvent(result=ev.query)\\nIf we run this:\\n\\nw = MainWorkflow(timeout=10, verbose=False)\\nresult = await w.run(query=\"Initial query\")\\nprint(result)\\nWe get:\\n\\nStarting up\\nSending an email\\nFinishing up\\nInitial query\\nNow let\\'s subclass this workflow to send a text message as well:\\n\\nclass Step2BEvent(Event):\\n    query: str\\n\\n\\nclass CustomWorkflow(MainWorkflow):\\n    @step\\n    async def step_two(self, ev: Step2Event) -> Step2BEvent:\\n        print(\"Sending an email\")\\n        return Step2BEvent(query=ev.query)\\n\\n    @step\\n    async def step_two_b(self, ev: Step2BEvent) -> Step3Event:\\n        print(\"Also sending a text message\")\\n        return Step3Event(query=ev.query)\\nWhich will instead give us\\n\\nStarting up\\nSending an email\\nAlso sending a text message\\nFinishing up\\nInitial query\\nWe can visualize the subclassed workflow and it will show all the steps, like this:\\n\\ndraw_all_possible_flows(CustomWorkflow, \"custom_workflow.html\")\\n\\n\\nNext, let\\'s look at another way to extend a workflow: [nested workflows](https://docs.llamaindex.ai/en/stable/understanding/workflows/nested/).\\n\\n\\n'}]}\n"
          ]
        }
      ],
      "source": [
        "url = \"https://api.usescraper.com/crawler/jobs/{}/data\".format(response[\"id\"])\n",
        "\n",
        "data_res = requests.request(\"GET\", url, headers=headers)\n",
        "\n",
        "data_res = json.loads(data_res.text)\n",
        "\n",
        "print(data_res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8VEQvJkITLJ",
        "outputId": "5661c4c4-a2bf-4f80-d6b8-acb213d71338"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "URL: https://docs.llamaindex.ai/en/stable/understanding/agent/multi_agents/\n",
            "Title: Multi-agent workflows - LlamaIndex\n",
            "Content:  \n",
            "[ Skip to content ](https://docs.llamaindex.ai/en/stable/understanding/agent/multi_agents/#multi-agent-workflows)\n",
            "#Multi-Agent Workflows[#](https://docs.llamaindex.ai/en/stable/understanding/agent/multi_agents/#multi-agent-workflows)\n",
            "The AgentWorkflow uses Workflow Agents to allow you to create a system of one or more agents that can collaborate and hand off tasks to each other based on their specialized capabilities. This enables building complex agent systems where different agents handle di ...\n"
          ]
        }
      ],
      "source": [
        "print(\"URL:\", data_res[\"data\"][0][\"meta\"][\"url\"])\n",
        "print(\"Title:\", data_res[\"data\"][0][\"meta\"][\"meta\"][\"title\"])\n",
        "print(\"Content:\", data_res[\"data\"][0][\"text\"][0:500], \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rt2nyuLhSYLR"
      },
      "source": [
        "## Convert to Document\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEieGzSFSXas"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.schema import Document\n",
        "\n",
        "# Convert the chunks to Document objects so the LlamaIndex framework can process them.\n",
        "documents = [\n",
        "    Document(\n",
        "        text=row[\"text\"],\n",
        "        metadata={\"title\": row[\"meta\"][\"meta\"][\"title\"], \"url\": row[\"meta\"][\"url\"]},\n",
        "    )\n",
        "    for row in data_res[\"data\"]\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqbJG5a1i3Jo"
      },
      "source": [
        "# Create RAG Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxmiQDv3SXV6"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "llm = OpenAI(temperature=0, model=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCVhv4OkSXTV"
      },
      "outputs": [],
      "source": [
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quwJI61dNVr-"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "text_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KpeCRMBUgup"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Settings\n",
        "\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model\n",
        "Settings.text_splitter = text_splitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWTBidwoZSO0"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "index = VectorStoreIndex.from_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUuJO0IIYSeU"
      },
      "outputs": [],
      "source": [
        "query_engine = index.as_query_engine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_s2LkH6YX1V"
      },
      "outputs": [],
      "source": [
        "res = query_engine.query(\"What is a query engine?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "02zdJNqIZKep",
        "outputId": "cd60ed02-eee2-49e6-c326-e3566cb868e9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A query engine is a component that processes and executes queries against a data source, retrieving relevant information based on specified criteria. It typically works in conjunction with indexing strategies to enhance the relevance, speed, and accuracy of the data retrieval process.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "res.response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuCcgP0nZSIl",
        "outputId": "289b4ace-b9d8-48b9-d708-85355ecaa40e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node ID\t ae9c2540-f0f0-44b8-aa78-615a944f547b\n",
            "Title\t Building an LLM Application - LlamaIndex\n",
            "URL\t https://docs.llamaindex.ai/en/stable/understanding/\n",
            "Score\t 0.3028895128495175\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Node ID\t 2a4b3a83-8904-4653-9472-5f28db53d30a\n",
            "Title\t Building an LLM Application - LlamaIndex\n",
            "URL\t https://docs.llamaindex.ai/en/stable/understanding/\n",
            "Score\t 0.24641542534057687\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
          ]
        }
      ],
      "source": [
        "# Show the retrieved nodes\n",
        "for src in res.source_nodes:\n",
        "    print(\"Node ID\\t\", src.node_id)\n",
        "    print(\"Title\\t\", src.metadata[\"title\"])\n",
        "    print(\"URL\\t\", src.metadata[\"url\"])\n",
        "    print(\"Score\\t\", src.score)\n",
        "    print(\"-_\" * 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6vplMnZ8VZM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}