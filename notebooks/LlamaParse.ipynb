{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ujwalkr94/LLM_Developer/blob/main/notebooks/LlamaParse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3iSJpxmEML0w",
        "outputId": "195968b6-ff54-422f-c662-e5025bef10ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.8/266.8 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.2/304.2 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q llama-index==0.12.36 llama-parse==0.6.25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9OKejSAM1Kj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# API access to llama-cloud\n",
        "os.environ[\"LLAMA_CLOUD_API_KEY\"] = \"llx-\"\n",
        "\n",
        "# from google.colab import userdata\n",
        "# os.environ[\"LLAMA_CLOUD_API_KEY\"] = userdata.get('LlamaParse_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "90d1105e699f44069f167b90d920f3f0",
            "56036efff9af4e7697eaba02c879992d",
            "1adab535cd1d417fa1f36e662156b01f",
            "4050fb987c73447280da416f607ed950",
            "fa8a5375db6046348bd765c3072d149d",
            "02992b306af44747a1c0465800c7cc95",
            "d4863d0ceb6b4430b46dff04ffd49579",
            "5b80847f9a694e8cb91b7e5a5ab5d341",
            "fef572f54ec64e5aa39c59e8e00a3593",
            "7f7ce9d9a0124330978a3e11ce999568",
            "5e9ea253404d4321828d6862d4e2e23f"
          ]
        },
        "id": "Xs5WjC01NCiy",
        "outputId": "f16d2e56-1e1d-487d-f71e-09f0d489d727"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "research_papers_llamaparse.zip:   0%|          | 0.00/13.6M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "90d1105e699f44069f167b90d920f3f0"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Downloading Research paper dataset from HuggingFace Hub\n",
        "from huggingface_hub import hf_hub_download\n",
        "file_path = hf_hub_download(repo_id=\"jaiganesan/ai_tutor_knowledge\", filename=\"research_papers_llamaparse.zip\",repo_type=\"dataset\",local_dir=\"/content\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRNqM8cIPQC_",
        "outputId": "25b48a69-3b05-473f-e809-3dfc67c9ecac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  research_papers_llamaparse.zip\n",
            "   creating: research_papers_llamaparse/\n",
            "  inflating: research_papers_llamaparse/2106.09685v2.pdf  \n",
            "  inflating: research_papers_llamaparse/2404.19756v2.pdf  \n",
            "  inflating: research_papers_llamaparse/2405.07437v2.pdf  \n"
          ]
        }
      ],
      "source": [
        "!unzip research_papers_llamaparse.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itMgf7Dsptw4"
      },
      "source": [
        "## Parse directory to LlamaParse"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "uN0oFRsGTcIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LlamaParse Implemetation\n",
        "from llama_parse import LlamaParse\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "#Parser\n",
        "parser = LlamaParse(\n",
        "    result_type=\"markdown\",\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "id": "vNInOZ3rbPSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir=\"/content/research_papers_llamaparse\"\n",
        "\n",
        "file_paths = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.lower().endswith(\".pdf\")]\n",
        "\n",
        "documents = await parser.aparse(file_paths)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qv_pyynbWXi",
        "outputId": "c3c85dc7-585b-4738-8e38-296188f9208c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGetting job results:   0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Started parsing the file under job_id a8a90c2d-37de-4144-b106-723332e186d1\n",
            "Started parsing the file under job_id c47244dc-4f8f-4c62-927b-62c9fa2de496\n",
            "Started parsing the file under job_id 02d2e426-34d1-4263-88ac-8cf435631efe\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Getting job results: 100%|██████████| 3/3 [00:30<00:00, 10.26s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(documents[0].pages[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9AE-vHW20B0",
        "outputId": "39e221d5-4f93-471f-e5e7-ec18c0aad98d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                            Evaluation of Retrieval-Augmented Generation:\n",
            "                                                                               A Survey\n",
            "                                 Hao Yu1,2, Aoran Gan3, Kai Zhang3, Shiwei Tong1†, Qi Liu3, and Zhaofeng Liu1\n",
            "                                                                1  Tencent Company\n",
            "                                                                2  McGill University\n",
            "                                                    3 State Key Laboratory of Cognitive Intelligence,\n",
            "                                                     University of Science and Technology of China\n",
            "arXiv:2405.07437v2  [cs.CL]  3 Jul 2024\n",
            "                                                                        hao.yu2@mail.mcgill.ca\n",
            "                                                                         gar@mail.ustc.edu.cn\n",
            "                                                 {shiweitong†,zhaofengliu}@tencent.com\n",
            "                                                                   {kkzhang08,qiliuql}@ustc.edu.cn\n",
            "                                      Abstract. Retrieval-Augmented Generation (RAG) has recently gained traction\n",
            "                                             in natural language processing. Numerous studies and real-world applications\n",
            "                                      are leveraging its ability to enhance generative models through external informa-\n",
            "                                            tion retrieval. Evaluating these RAG systems, however, poses unique challenges\n",
            "                                             due to their hybrid structure and reliance on dynamic knowledge sources. To\n",
            "                                            better understand these challenges, we conduct A Unified Evaluation Process of\n",
            "                                             RAG (Auepora) and aim to provide a comprehensive overview of the evaluation\n",
            "                                             and benchmarks of RAG systems. Specifically, we examine and compare several\n",
            "                                      quantifiable metrics of the Retrieval and Generation components, such as rele-\n",
            "                                             vance, accuracy, and faithfulness, within the current RAG benchmarks, encom-\n",
            "                                           passing the possible output and ground truth pairs. We then analyze the various\n",
            "                                           datasets and metrics, discuss the limitations of current benchmarks, and suggest\n",
            "                                      potential directions to advance the field of RAG benchmarks.\n",
            "                               1   Introduction\n",
            "                                          Retrieval-Augmented Generation (RAG) [34] efficiently enhances the performance of\n",
            "                                       generative language models through integrating information retrieval techniques. It ad-\n",
            "                               dresses a critical challenge faced by standalone generative language models: the ten-\n",
            "                                          dency to produce responses that, while plausible, may not be grounded in facts. By\n",
            "                                         retrieving relevant information from external sources, RAG significantly reduces the\n",
            "                                        incidence of hallucinations [23] or factually incorrect outputs, thereby improving the\n",
            "                                      content’s reliability and richness. [73] This fusion of retrieval and generation capabil-\n",
            "                                        ities enables the creation of responses that are not only contextually appropriate but\n",
            "                               also informed by the most current and accurate information available, making RAG a\n",
            "                               development in the pursuit of more intelligent and versatile language models [73,64].\n",
            "                                † Corresponding Author\n",
            "                                 Paper Homepage: https://github.com/YHPeter/Awesome-RAG-Evaluation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCfzTVX0pqhs"
      },
      "source": [
        "## LlamaParse JSON Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBb4VjgPPiWq",
        "outputId": "4dd22621-0101-4aba-f146-6831c87ee145"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rParsing files:   0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Started parsing the file under job_id 557abae2-17df-4faf-8f59-7c1b224b713e\n",
            "Started parsing the file under job_id ffe15d11-ffd8-4fb6-aa0c-bad8bd4bf341\n",
            "Started parsing the file under job_id 5577d117-ddf1-4d77-84f9-bcfabf5ffa62\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Parsing files: 100%|██████████| 3/3 [00:19<00:00,  6.40s/it]\n"
          ]
        }
      ],
      "source": [
        "# Using LlamaParse in JSON Mode for PDF Reading\n",
        "\n",
        "import glob\n",
        "pdf_files = glob.glob(\"/content/research_papers_llamaparse/*.pdf\")\n",
        "\n",
        "parser = LlamaParse(verbose=True)\n",
        "\n",
        "json_objs=parser.get_json_result(pdf_files)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "json_objs[0]['pages'][1]['text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "WUwCG9Xb4WoY",
        "outputId": "a48e9ecc-72de-41b1-a5ca-df036d8b9c80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2        Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong†, Qi Liu, and Zhaofeng Liu\\n   Numerous studies of RAG systems have emerged from various perspectives since\\nthe advent of Large Language Models (LLMs) [55,45,59,42,41,69,16]. The RAG sys-\\ntem comprises two primary components: Retrieval and Generation. The retrieval com-\\nponent aims to extract relevant information from various external knowledge sources.\\nIt involves two main phases, indexing and searching. Indexing organizes documents to\\nfacilitate efficient retrieval, using either inverted indexes for sparse retrieval or dense\\nvector encoding for dense retrieval [16,12,28]. The searching component utilizes these\\nindexes to fetch relevant documents on the user’s query, often incorporating the op-\\ntional rerankers [4,39,6,52] to refine the ranking of the retrieved documents. The gener-\\nation component utilizes the retrieved content and question query to formulate coherent\\nand contextually relevant responses with the prompting and inferencing phases. As the\\n“Emerging” ability [59] of LLMs and the breakthrough in aligning human commands\\n[42], LLMs are the best performance choices model for the generation stage. Prompt-\\ning methods like Chain of Thought (CoT) [60], Tree of Thgouht [65], Rephrase and\\nRespond (RaR) [8] guide better generation results. In the inferencing step, LLMs inter-\\npret the prompted input to generate accurate and in-depth responses that align with the\\nquery’s intent and integrate the extracted information [35,9] without further finetuning,\\nsuch as fully finetuning [16,1,67,68] or LoRA [21]. Appendix A details the complete\\nRAG structure. Figure 1 illustrates the structure of the RAG systems as mentioned.\\n                     Retrieval                     Generation           Ground Truth\\n                              II. Search      III. Prompting\\n          Web Search Engine  1                  Query   1\\n                 BM25           Relevant      Relevant Docs  Complete\\n  Query        KNN/ANN         Docuseort|     System Prompt  Prompt\\n                                              Prompt Skills\\n                                                                      Docs Candidates\\n                                IDF             Large Language Model\\n             Wikipedia         Vector           Response              Sample Response\\n            HF Dataset           ES                   Post\\n                              Database               Processing             Label\\n                                                           Output\\n                               Indexing       IV. Inferencing\\nFig. 1: The structure of the RAG system with retrieval and generation components and\\ncorresponding four phrases: indexing, search, prompting and inferencing. The pairs of\\n“Evaluable Outputs” (EOs) and “Ground Truths” (GTs) are highlighted in read frame\\nand green frame, with brown dashed arrows.\\n   The importance of evaluating RAG is increasing in parallel with the advancement\\nof RAG-specific methodologies. On the one hand, RAG is a complex system intricately\\ntied to specific requirements and language models, resulting in various evaluation meth-\\nods, indicators, and tools, particularly given the black-box LLM generation. Evaluating\\nRAG systems involves specific components and the complexity of the overall system\\nassessment. On the other hand, the complexity of RAG systems is further compounded'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "fN075z18fAHI",
        "outputId": "556094f6-5e58-4c7f-b4e7-517609c90e11"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'                          Evaluation of Retrieval-Augmented Generation: A Survey     5\\n                  Result     Query   Ground Truth\\n Retrieval     Relevant Docs       Docs Candidates   Relevant Docs  Query    Relevance\\n                                                     Relevant Docs  Docs Candidates             Accuracy\\n                Response           Sample Response   Response  Query                           Relevance\\n Generation      Output                              Response  Relevant Docs  Faithfulness\\n                                        Label        Response  Sample Response               Correctness\\n Additional Requirements                      Latency, Noise Robustness Negative Rejection, Diversity,..\\n                      Fig. 2: The Target modular of the Auepora.\\n3.1  Evaluation Target (What to Evaluate?)\\nThe combination of EOs and GTs in the RAG system can generate all possible targets,\\nwhich is the fundamental concept of the Auepora (as shown in Figure 1). Once iden-\\ntified, these targets can be defined based on a specific pair of EOs or EO with GT, as\\nillustrated in Figure 2, and used to analyze all aspects of current RAG benchmarks.\\nRetrieval  The EOs are the relevant documents for evaluating the retrieval component\\ndepending on the query. Then we can construct two pairwise relationships for the re-\\ntrieval component, which are Relevant Documents ↔ Query, Relevant Documents ↔\\nDocuments Candidates.\\n  -  Relevance (Relevant Documents ↔ Query) evaluates how well the retrieved docu-\\n     ments match the information needed expressed in the query. It measures the preci-\\n     sion and specificity of the retrieval process.\\n  -  Accuracy (Relevant Documents ↔ Documents Candidates) assesses how accurate\\n     the retrieved documents are in comparison to a set of candidate documents. It is\\n     a measure of the system’s ability to identify and score relevant documents higher\\n     than less relevant or irrelevant ones.\\nGeneration   The similar pairwise relations for the generation components are listed\\nbelow. The EOs are the generated text and phrased structured content. Then we need to\\ncompare these EOs with the provided GTs and labels.\\n  -  Relevance (Response ↔ Query) measures how well the generated response aligns\\n     with the intent and content of the initial query. It ensures that the response is related\\n     to the query topic and meets the query’s specific requirements.\\n  -  Faithfulness (Response ↔ Relevant Documents) evaluates if the generated re-\\n     sponse accurately reflects the information contained within the relevant documents\\n     and measures the consistency between generated content and the source documents.\\n  -  Correctness  (Response ↔ Sample Response) Similar to the accuracy in the re-\\n     trieval component, this measures the accuracy of the generated response against a\\n     sample response, which serves as a ground truth. It checks if the response is correct\\n     in terms of factual information and appropriate in the context of the query.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "json_objs[0]['pages'][4]['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEnTFCn2pgB7",
        "outputId": "a4e2a4f7-ca8b-4701-a76b-65ab1e8eb378"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'type': 'text',\n",
              " 'value': 'The presentation distinguishes between the core areas of Retrieval and Generation considered in the evaluation. The different aspects of the evaluation are set as different colours in the table: Relevance, Accuracy of Retrieval and Faithfulness, Correctness and Relevance of Generation. The consideration of the Additional Requirements beyond the retrieval and generation component is also collected. Noted that quite a few of the works employed multiple methods or evaluated multiple aspects simultaneously.',\n",
              " 'md': 'The presentation distinguishes between the core areas of Retrieval and Generation considered in the evaluation. The different aspects of the evaluation are set as different colours in the table: Relevance, Accuracy of Retrieval and Faithfulness, Correctness and Relevance of Generation. The consideration of the Additional Requirements beyond the retrieval and generation component is also collected. Noted that quite a few of the works employed multiple methods or evaluated multiple aspects simultaneously.',\n",
              " 'bBox': {'x': 134.76, 'y': 185.83, 'w': 345.62, 'h': 400.24}}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# Table information\n",
        "json_objs[0]['pages'][5]['items'][2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2KgsfBKrxME",
        "outputId": "caf88a76-ee2f-45de-caa5-09fb9569169e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'type': 'table',\n",
              " 'rows': [['Category',\n",
              "   'Framework',\n",
              "   'Time',\n",
              "   'Raw Targets',\n",
              "   'Retrieval',\n",
              "   'Generation'],\n",
              "  ['Tool',\n",
              "   'TruEra RAG Triad [54]',\n",
              "   '2023.10',\n",
              "   'Answer Relevance',\n",
              "   'LLM as a Judge',\n",
              "   'LLM as a Judge'],\n",
              "  ['', '', '', 'Groundedness', 'Accuracy', ''],\n",
              "  ['Tool',\n",
              "   'LangChain Bench. [32]',\n",
              "   '2023.11',\n",
              "   'Faithfulness',\n",
              "   'Accuracy',\n",
              "   'LLM as a Judge'],\n",
              "  ['', '', '', 'Execution Time', 'Embed. CosDistance', 'Correctness'],\n",
              "  ['Tool',\n",
              "   'Databricks Eval [33]',\n",
              "   '2023.12',\n",
              "   'Readability',\n",
              "   '-',\n",
              "   'LLM as a Judge'],\n",
              "  ['', '', '', 'Comprehensiveness', 'Context Relevance', 'LLM Gen + CosSim'],\n",
              "  ['Benchmark',\n",
              "   'RAGAs [14]',\n",
              "   '2023.09',\n",
              "   'Answer Relevance',\n",
              "   'LLM as a Judge',\n",
              "   'LLM as a Judge'],\n",
              "  ['', '', '', 'Faithfulness', '', ''],\n",
              "  ['Benchmark',\n",
              "   'RECALL [38]',\n",
              "   '2023.11',\n",
              "   'Response Quality',\n",
              "   '-',\n",
              "   'BLEU, ROUGE-L'],\n",
              "  ['', '', '', 'Robustness', 'Context Relevance', 'LLM + Classifier'],\n",
              "  ['Benchmark',\n",
              "   'ARES [49]',\n",
              "   '2023.11',\n",
              "   'Answer Faithfulness',\n",
              "   'LLM + Classifier',\n",
              "   'LLM + Classifier'],\n",
              "  ['', '', '', 'Answer Relevance', 'Information Integration', ''],\n",
              "  ['Benchmark', 'RGB [6]', '2023.12', 'Noise Robustness', '-', 'Accuracy'],\n",
              "  ['', '', '', 'Negative Rejection', 'Counterfactual Robustness', ''],\n",
              "  ['Benchmark',\n",
              "   'MultiHop-RAG [52]',\n",
              "   '2024.01',\n",
              "   'Retrieval Quality',\n",
              "   'MAP, MRR, Hit@K',\n",
              "   'LLM as a Judge'],\n",
              "  ['', '', '', 'Response Correctness', '', ''],\n",
              "  ['Benchmark',\n",
              "   'CRUD-RAG [39]',\n",
              "   '2024.02',\n",
              "   'CREATE, READ',\n",
              "   '-',\n",
              "   'ROUGE, BLEU'],\n",
              "  ['', '', '', 'UPDATE, DELETE', '', 'RAGQuestEval'],\n",
              "  ['Benchmark', 'MedRAG [61]', '2024.02', 'Accuracy', '-', 'Accuracy'],\n",
              "  ['', '', '', 'Consistency', '', ''],\n",
              "  ['Benchmark',\n",
              "   'FeB4RAG [57]',\n",
              "   '2024.02',\n",
              "   'Correctness',\n",
              "   '-',\n",
              "   'Human Evaluation'],\n",
              "  ['', '', '', 'Clarity', 'Coverage', ''],\n",
              "  ['Benchmark', 'CDQA [62]', '2024.03', 'Accuracy', '-', 'F1'],\n",
              "  ['', '', '', 'Correctness', '', 'F1, Exact-Match'],\n",
              "  ['Benchmark', 'DomainRAG [58]', '2024.06', 'Faithfulness', '-', 'Rouge-L'],\n",
              "  ['', '', '', 'Noise Robustness', 'LLM as a Judge', 'Structural Output'],\n",
              "  ['', '', '', '', 'F1, Exacct-Match', ''],\n",
              "  ['Benchmark',\n",
              "   'ReEval [66]',\n",
              "   '2024.06',\n",
              "   'Hallucination',\n",
              "   '-',\n",
              "   'LLM as a Judge'],\n",
              "  ['', '', '', '', 'Human Evaluation', ''],\n",
              "  ['Research', 'FiD-Light [20]', '2023.07', 'Latency', '-', '-'],\n",
              "  ['Research',\n",
              "   'Diversity Reranker [4]',\n",
              "   '2023.08',\n",
              "   'Diversity',\n",
              "   'Cosine Distance',\n",
              "   '']],\n",
              " 'md': '| Category  | Framework               | Time    | Raw Targets          | Retrieval                 | Generation        |\\n| --------- | ----------------------- | ------- | -------------------- | ------------------------- | ----------------- |\\n| Tool      | TruEra RAG Triad \\\\[54]  | 2023.10 | Answer Relevance     | LLM as a Judge            | LLM as a Judge    |\\n|           |                         |         | Groundedness         | Accuracy                  |                   |\\n| Tool      | LangChain Bench. \\\\[32]  | 2023.11 | Faithfulness         | Accuracy                  | LLM as a Judge    |\\n|           |                         |         | Execution Time       | Embed. CosDistance        | Correctness       |\\n| Tool      | Databricks Eval \\\\[33]   | 2023.12 | Readability          | -                         | LLM as a Judge    |\\n|           |                         |         | Comprehensiveness    | Context Relevance         | LLM Gen + CosSim  |\\n| Benchmark | RAGAs \\\\[14]             | 2023.09 | Answer Relevance     | LLM as a Judge            | LLM as a Judge    |\\n|           |                         |         | Faithfulness         |                           |                   |\\n| Benchmark | RECALL \\\\[38]            | 2023.11 | Response Quality     | -                         | BLEU, ROUGE-L     |\\n|           |                         |         | Robustness           | Context Relevance         | LLM + Classifier  |\\n| Benchmark | ARES \\\\[49]              | 2023.11 | Answer Faithfulness  | LLM + Classifier          | LLM + Classifier  |\\n|           |                         |         | Answer Relevance     | Information Integration   |                   |\\n| Benchmark | RGB \\\\[6]                | 2023.12 | Noise Robustness     | -                         | Accuracy          |\\n|           |                         |         | Negative Rejection   | Counterfactual Robustness |                   |\\n| Benchmark | MultiHop-RAG \\\\[52]      | 2024.01 | Retrieval Quality    | MAP, MRR, Hit\\\\@K          | LLM as a Judge    |\\n|           |                         |         | Response Correctness |                           |                   |\\n| Benchmark | CRUD-RAG \\\\[39]          | 2024.02 | CREATE, READ         | -                         | ROUGE, BLEU       |\\n|           |                         |         | UPDATE, DELETE       |                           | RAGQuestEval      |\\n| Benchmark | MedRAG \\\\[61]            | 2024.02 | Accuracy             | -                         | Accuracy          |\\n|           |                         |         | Consistency          |                           |                   |\\n| Benchmark | FeB4RAG \\\\[57]           | 2024.02 | Correctness          | -                         | Human Evaluation  |\\n|           |                         |         | Clarity              | Coverage                  |                   |\\n| Benchmark | CDQA \\\\[62]              | 2024.03 | Accuracy             | -                         | F1                |\\n|           |                         |         | Correctness          |                           | F1, Exact-Match   |\\n| Benchmark | DomainRAG \\\\[58]         | 2024.06 | Faithfulness         | -                         | Rouge-L           |\\n|           |                         |         | Noise Robustness     | LLM as a Judge            | Structural Output |\\n|           |                         |         |                      | F1, Exacct-Match          |                   |\\n| Benchmark | ReEval \\\\[66]            | 2024.06 | Hallucination        | -                         | LLM as a Judge    |\\n|           |                         |         |                      | Human Evaluation          |                   |\\n| Research  | FiD-Light \\\\[20]         | 2023.07 | Latency              | -                         | -                 |\\n| Research  | Diversity Reranker \\\\[4] | 2023.08 | Diversity            | Cosine Distance           |                   |',\n",
              " 'isPerfectTable': True,\n",
              " 'csv': '\"Category\",\"Framework\",\"Time\",\"Raw Targets\",\"Retrieval\",\"Generation\"\\n\"Tool\",\"TruEra RAG Triad [54]\",\"2023.10\",\"Answer Relevance\",\"LLM as a Judge\",\"LLM as a Judge\"\\n\"\",\"\",\"\",\"Groundedness\",\"Accuracy\",\"\"\\n\"Tool\",\"LangChain Bench. [32]\",\"2023.11\",\"Faithfulness\",\"Accuracy\",\"LLM as a Judge\"\\n\"\",\"\",\"\",\"Execution Time\",\"Embed. CosDistance\",\"Correctness\"\\n\"Tool\",\"Databricks Eval [33]\",\"2023.12\",\"Readability\",\"-\",\"LLM as a Judge\"\\n\"\",\"\",\"\",\"Comprehensiveness\",\"Context Relevance\",\"LLM Gen + CosSim\"\\n\"Benchmark\",\"RAGAs [14]\",\"2023.09\",\"Answer Relevance\",\"LLM as a Judge\",\"LLM as a Judge\"\\n\"\",\"\",\"\",\"Faithfulness\",\"\",\"\"\\n\"Benchmark\",\"RECALL [38]\",\"2023.11\",\"Response Quality\",\"-\",\"BLEU, ROUGE-L\"\\n\"\",\"\",\"\",\"Robustness\",\"Context Relevance\",\"LLM + Classifier\"\\n\"Benchmark\",\"ARES [49]\",\"2023.11\",\"Answer Faithfulness\",\"LLM + Classifier\",\"LLM + Classifier\"\\n\"\",\"\",\"\",\"Answer Relevance\",\"Information Integration\",\"\"\\n\"Benchmark\",\"RGB [6]\",\"2023.12\",\"Noise Robustness\",\"-\",\"Accuracy\"\\n\"\",\"\",\"\",\"Negative Rejection\",\"Counterfactual Robustness\",\"\"\\n\"Benchmark\",\"MultiHop-RAG [52]\",\"2024.01\",\"Retrieval Quality\",\"MAP, MRR, Hit@K\",\"LLM as a Judge\"\\n\"\",\"\",\"\",\"Response Correctness\",\"\",\"\"\\n\"Benchmark\",\"CRUD-RAG [39]\",\"2024.02\",\"CREATE, READ\",\"-\",\"ROUGE, BLEU\"\\n\"\",\"\",\"\",\"UPDATE, DELETE\",\"\",\"RAGQuestEval\"\\n\"Benchmark\",\"MedRAG [61]\",\"2024.02\",\"Accuracy\",\"-\",\"Accuracy\"\\n\"\",\"\",\"\",\"Consistency\",\"\",\"\"\\n\"Benchmark\",\"FeB4RAG [57]\",\"2024.02\",\"Correctness\",\"-\",\"Human Evaluation\"\\n\"\",\"\",\"\",\"Clarity\",\"Coverage\",\"\"\\n\"Benchmark\",\"CDQA [62]\",\"2024.03\",\"Accuracy\",\"-\",\"F1\"\\n\"\",\"\",\"\",\"Correctness\",\"\",\"F1, Exact-Match\"\\n\"Benchmark\",\"DomainRAG [58]\",\"2024.06\",\"Faithfulness\",\"-\",\"Rouge-L\"\\n\"\",\"\",\"\",\"Noise Robustness\",\"LLM as a Judge\",\"Structural Output\"\\n\"\",\"\",\"\",\"\",\"F1, Exacct-Match\",\"\"\\n\"Benchmark\",\"ReEval [66]\",\"2024.06\",\"Hallucination\",\"-\",\"LLM as a Judge\"\\n\"\",\"\",\"\",\"\",\"Human Evaluation\",\"\"\\n\"Research\",\"FiD-Light [20]\",\"2023.07\",\"Latency\",\"-\",\"-\"\\n\"Research\",\"Diversity Reranker [4]\",\"2023.08\",\"Diversity\",\"Cosine Distance\",\"\"',\n",
              " 'bBox': {'x': 134.76, 'y': 90.93, 'w': 345.77, 'h': 565.73}}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "json_objs[0]['pages'][5]['items'][3]"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CkkzplqfxnGr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "90d1105e699f44069f167b90d920f3f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_56036efff9af4e7697eaba02c879992d",
              "IPY_MODEL_1adab535cd1d417fa1f36e662156b01f",
              "IPY_MODEL_4050fb987c73447280da416f607ed950"
            ],
            "layout": "IPY_MODEL_fa8a5375db6046348bd765c3072d149d"
          }
        },
        "56036efff9af4e7697eaba02c879992d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02992b306af44747a1c0465800c7cc95",
            "placeholder": "​",
            "style": "IPY_MODEL_d4863d0ceb6b4430b46dff04ffd49579",
            "value": "research_papers_llamaparse.zip: 100%"
          }
        },
        "1adab535cd1d417fa1f36e662156b01f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b80847f9a694e8cb91b7e5a5ab5d341",
            "max": 13584308,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fef572f54ec64e5aa39c59e8e00a3593",
            "value": 13584308
          }
        },
        "4050fb987c73447280da416f607ed950": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f7ce9d9a0124330978a3e11ce999568",
            "placeholder": "​",
            "style": "IPY_MODEL_5e9ea253404d4321828d6862d4e2e23f",
            "value": " 13.6M/13.6M [00:00&lt;00:00, 33.6MB/s]"
          }
        },
        "fa8a5375db6046348bd765c3072d149d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02992b306af44747a1c0465800c7cc95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4863d0ceb6b4430b46dff04ffd49579": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b80847f9a694e8cb91b7e5a5ab5d341": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fef572f54ec64e5aa39c59e8e00a3593": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7f7ce9d9a0124330978a3e11ce999568": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e9ea253404d4321828d6862d4e2e23f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}