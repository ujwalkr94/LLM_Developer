{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ujwalkr94/LLM_Developer/blob/main/notebooks/Firecrawl_Scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9h3lrtQRhYv"
      },
      "source": [
        "## Install the requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKBQrqNsjrLB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "e94d488b-e3fd-4e9b-ba72-1b8e7fe70e47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/56.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.2/80.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.8/266.8 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/45.1 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.2/304.2 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for html2text (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for spider-client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "typeguard 4.4.3 requires typing_extensions>=4.14.0, but you have typing-extensions 4.13.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q llama-index==0.12.12 openai==1.59.6 tiktoken==0.8.0 llama-index-readers-web==0.3.4 firecrawl-py==1.10.1\n",
        "\n",
        "# (OR) To resolve the dependency issue.\n",
        "# !pip uninstall -q torch torchvision torchaudio\n",
        "# !pip install -q torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu126\n",
        "# !pip install -q llama-index==0.12.12 openai==1.59.6 tiktoken==0.8.0 llama-index-readers-web==0.3.4 firecrawl-py==1.10.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKh_dKV6Rm9a"
      },
      "source": [
        "### SET THE ENVIRONMENT VARIABLES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZgBdtZRJfze"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"<OPENAI_API_KEY>\"\n",
        "FIRECRAWL_API_KEY = \"<FIRECRAWL_API_KEY>\"\n",
        "\n",
        "# from google.colab import userdata\n",
        "# os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY1')\n",
        "# os.environ[\"FIRECRAWL_API_KEY\"] = userdata.get('FIRECRAWL_API_KEY1')\n",
        "\n",
        "# FIRECRAWL_API_KEY = userdata.get('FIRECRAWL_API_KEY1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiBy7Q74Vkt_"
      },
      "source": [
        "# SCRAPE WITH FIRECRAWL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQP8VA_gRr2g"
      },
      "source": [
        "## IMPORT THE FIRECRAWL WEBREADER\n",
        "\n",
        "Firecrawl allows you to turn entire websites into LLM-ready markdown\n",
        "\n",
        "Get the API key here\n",
        "https://www.firecrawl.dev/app/api-keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YuLILEoNj2lG"
      },
      "outputs": [],
      "source": [
        "from llama_index.readers.web import FireCrawlWebReader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-ls9L0QWC3S"
      },
      "outputs": [],
      "source": [
        "\n",
        "# using firecrawl to crawl a website\n",
        "firecrawl_reader = FireCrawlWebReader(\n",
        "    api_key=FIRECRAWL_API_KEY,  # Replace with your actual API key from https://www.firecrawl.dev/\n",
        "    mode=\"scrape\",\n",
        ")\n",
        "\n",
        "# Load documents from a single page URL\n",
        "documents = firecrawl_reader.load_data(url=\"https://towardsai.net/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zL1BOBWhWrfe"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex, Settings\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "# To increase chunk size globally\n",
        "# Settings.chunk_size = 2048  # or even larger like 4096\n",
        "# Settings.chunk_overlap = 200\n",
        "\n",
        "\n",
        "# node parser with larger chunk size only for this index\n",
        "node_parser = SentenceSplitter(\n",
        "    chunk_size=2048,\n",
        "    chunk_overlap=200,\n",
        ")\n",
        "\n",
        "index = VectorStoreIndex.from_documents(documents, transformations=[node_parser])\n",
        "query_engine = index.as_query_engine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AV4Fg-haWspw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95e74472-9a86-4b0a-a08e-74a6d51237b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Towards AI aims to be the leading AI community and content platform that makes AI accessible to all by providing high-quality publications, news, articles, and stories on AI and technology-related topics.\n",
            "-----------------\n",
            "Node ID\t b363bfbe-e859-4d62-9dac-8d85ce6613e3\n",
            "Title\t Towards AI\n",
            "URL\t https://towardsai.net/\n",
            "Score\t 0.8576130818181417\n",
            "Description\t Towards AI is an online publication, which focuses on sharing high-quality publications, news, articles, and stories on AI and technology related topics., Louie's thoughts on the week's biggest AI developments. \n",
            "All major AI news, models, tools and papers covered. \n",
            "Read by over 130,000 AI Practitioners, Industry Professionals and Students. Click to read Towards AI Newsletter, a Substack publication.\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Node ID\t 602747b8-4964-408d-a5fc-307e73c4c4e2\n",
            "Title\t Towards AI\n",
            "URL\t https://towardsai.net/\n",
            "Score\t 0.8493605511744174\n",
            "Description\t Towards AI is an online publication, which focuses on sharing high-quality publications, news, articles, and stories on AI and technology related topics., Louie's thoughts on the week's biggest AI developments. \n",
            "All major AI news, models, tools and papers covered. \n",
            "Read by over 130,000 AI Practitioners, Industry Professionals and Students. Click to read Towards AI Newsletter, a Substack publication.\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
          ]
        }
      ],
      "source": [
        "res = query_engine.query(\"What is towards AI aim?\")\n",
        "\n",
        "print(res.response)\n",
        "\n",
        "print(\"-----------------\")\n",
        "# Show the retrieved nodes\n",
        "for src in res.source_nodes:\n",
        "  print(\"Node ID\\t\", src.node_id)\n",
        "  print(\"Title\\t\", src.metadata['title'])\n",
        "  print(\"URL\\t\", src.metadata['sourceURL'])\n",
        "  print(\"Score\\t\", src.score)\n",
        "  print(\"Description\\t\", src.metadata.get(\"description\"))\n",
        "  print(\"-_\"*20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRjJrc7VVaNX"
      },
      "source": [
        "# CRAWL A WEBSITE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uj6dqla8SEwd"
      },
      "source": [
        "## Load The CSV\n",
        "\n",
        "CSV contains the list of tools and url of the page which we use to get information about the tool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWTgax6zZpLx"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import csv\n",
        "\n",
        "# Google Sheets file URL (CSV export link)\n",
        "url = 'https://docs.google.com/spreadsheets/d/1gHB-aQJGt9Nl3cyOP2GorAkBI_Us2AqkYnfqrmejStc/export?format=csv'\n",
        "\n",
        "# Send a GET request to fetch the CSV file\n",
        "response = requests.get(url)\n",
        "\n",
        "response_list = []\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    # Decode the content to a string\n",
        "    content = response.content.decode('utf-8')\n",
        "\n",
        "    # Use the csv.DictReader to read the content as a dictionary\n",
        "    csv_reader = csv.DictReader(content.splitlines(), delimiter=',')\n",
        "    response_list = [row for row in csv_reader]\n",
        "else:\n",
        "    print(f\"Failed to retrieve the file: {response.status_code}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atFrRz4MgmaR"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "start_index = random.randint(0, len(response_list) - 3)\n",
        "website_list = response_list[start_index:start_index+10] # Crawling 10 websites only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHWjBFSQMWZk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4caf2531-399c-43e0-fce2-56a5a35531d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV data\n",
            "[{'': '',\n",
            "  'Category': 'Data Serialization',\n",
            "  'Company': '',\n",
            "  'Description': 'Data serialization system',\n",
            "  'Is a direct URL company /tool website?': 'Yes',\n",
            "  'Name': 'Apache Avro',\n",
            "  'Parent': 'Apache',\n",
            "  'Tool Type': 'Format/Algorithm/System',\n",
            "  'URL': 'https://avro.apache.org/'},\n",
            " {'': '',\n",
            "  'Category': 'Data Serialization',\n",
            "  'Company': '',\n",
            "  'Description': 'Insanely fast data interchange format and capability-based '\n",
            "                 'RPC system',\n",
            "  'Is a direct URL company /tool website?': 'Yes',\n",
            "  'Name': \"Cap'n Proto\",\n",
            "  'Parent': '',\n",
            "  'Tool Type': 'Format/Algorithm/System',\n",
            "  'URL': 'https://capnproto.org/'},\n",
            " {'': '',\n",
            "  'Category': 'Data Storage',\n",
            "  'Company': '',\n",
            "  'Description': 'Fast, lightweight binary columnar data format',\n",
            "  'Is a direct URL company /tool website?': 'No',\n",
            "  'Name': 'Feather',\n",
            "  'Parent': '',\n",
            "  'Tool Type': 'Format/Algorithm/System',\n",
            "  'URL': 'https://arrow.apache.org/docs/python/feather.html'},\n",
            " {'': '',\n",
            "  'Category': 'Data Storage',\n",
            "  'Company': '',\n",
            "  'Description': 'Data model, library, and file format for storing and '\n",
            "                 'managing data',\n",
            "  'Is a direct URL company /tool website?': 'No',\n",
            "  'Name': 'HDF5',\n",
            "  'Parent': '',\n",
            "  'Tool Type': 'Format/Algorithm/System',\n",
            "  'URL': 'https://www.neonscience.org/resources/learning-hub/tutorials/about-hdf5'},\n",
            " {'': '',\n",
            "  'Category': 'Data Serialization',\n",
            "  'Company': '',\n",
            "  'Description': 'Efficient binary serialization format',\n",
            "  'Is a direct URL company /tool website?': 'Yes',\n",
            "  'Name': 'MessagePack',\n",
            "  'Parent': '',\n",
            "  'Tool Type': 'Format/Algorithm/System',\n",
            "  'URL': 'https://msgpack.org/index.html'},\n",
            " {'': '',\n",
            "  'Category': 'Data Storage',\n",
            "  'Company': '',\n",
            "  'Description': 'Columnar storage format available to any project in the '\n",
            "                 'Hadoop ecosystem',\n",
            "  'Is a direct URL company /tool website?': 'Yes',\n",
            "  'Name': 'Parquet',\n",
            "  'Parent': '',\n",
            "  'Tool Type': 'Format/Algorithm/System',\n",
            "  'URL': 'https://parquet.apache.org/'},\n",
            " {'': '',\n",
            "  'Category': 'Data Serialization',\n",
            "  'Company': '',\n",
            "  'Description': \"Google's language-neutral, platform-neutral, extensible \"\n",
            "                 'mechanism for serializing structured data',\n",
            "  'Is a direct URL company /tool website?': 'No',\n",
            "  'Name': 'Protocol Buffers',\n",
            "  'Parent': '',\n",
            "  'Tool Type': 'Format/Algorithm/System',\n",
            "  'URL': 'https://protobuf.dev/overview/#:~:text=Protocol%20Buffers%20are%20a%20language,it%20generates%20native%20language%20bindings.'},\n",
            " {'': '',\n",
            "  'Category': 'Data Storage',\n",
            "  'Company': '',\n",
            "  'Description': 'Format for the storage of chunked, compressed, N-dimensional '\n",
            "                 'arrays',\n",
            "  'Is a direct URL company /tool website?': 'Yes',\n",
            "  'Name': 'Zarr',\n",
            "  'Parent': '',\n",
            "  'Tool Type': 'Format/Algorithm/System',\n",
            "  'URL': 'https://zarr.dev/'},\n",
            " {'': '',\n",
            "  'Category': 'Hyperparameter Tuning',\n",
            "  'Company': '',\n",
            "  'Description': 'Bayesian Optimization and HyperBand',\n",
            "  'Is a direct URL company /tool website?': 'No',\n",
            "  'Name': 'BOHB',\n",
            "  'Parent': '',\n",
            "  'Tool Type': 'Format/Algorithm/System',\n",
            "  'URL': 'https://dzone.com/articles/bayesian-optimization-and-hyperband-bohb-hyperpara#:~:text=Bayesian%20Optimization%20and%20Hyperband%20(BOHB)%20is%20a%20cutting%2Dedge,hyperparameters%20for%20machine%20learning%20models.'},\n",
            " {'': '',\n",
            "  'Category': 'Message Queue',\n",
            "  'Company': '',\n",
            "  'Description': 'Cloud native messaging system',\n",
            "  'Is a direct URL company /tool website?': 'Yes',\n",
            "  'Name': 'NATS',\n",
            "  'Parent': '',\n",
            "  'Tool Type': 'Format/Algorithm/System',\n",
            "  'URL': 'https://nats.io/'}]\n"
          ]
        }
      ],
      "source": [
        "import pprint\n",
        "print(\"CSV data\")\n",
        "pprint.pprint(website_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1SO6txzTBT_"
      },
      "source": [
        "## Initialize the Firecrawl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8GphTUy7IS_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from firecrawl import FirecrawlApp\n",
        "app = FirecrawlApp(api_key=FIRECRAWL_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-G8fB6KyGP7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd3fd0bd-2de5-456f-d0ad-8ba0c8e76b58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Crawling: https://avro.apache.org/\n",
            "Pausing for 1 minute to comply with crawl limit...\n",
            "Crawling: https://capnproto.org/\n",
            "Pausing for 1 minute to comply with crawl limit...\n",
            "Crawling: https://arrow.apache.org/docs/python/feather.html\n",
            "Pausing for 1 minute to comply with crawl limit...\n",
            "Crawling: https://www.neonscience.org/resources/learning-hub/tutorials/about-hdf5\n",
            "Pausing for 1 minute to comply with crawl limit...\n",
            "Crawling: https://msgpack.org/index.html\n",
            "Pausing for 1 minute to comply with crawl limit...\n",
            "Crawling: https://parquet.apache.org/\n",
            "Pausing for 1 minute to comply with crawl limit...\n",
            "Crawling: https://protobuf.dev/overview/#:~:text=Protocol%20Buffers%20are%20a%20language,it%20generates%20native%20language%20bindings.\n",
            "Pausing for 1 minute to comply with crawl limit...\n",
            "Crawling: https://zarr.dev/\n",
            "Pausing for 1 minute to comply with crawl limit...\n",
            "Crawling: https://dzone.com/articles/bayesian-optimization-and-hyperband-bohb-hyperpara#:~:text=Bayesian%20Optimization%20and%20Hyperband%20(BOHB)%20is%20a%20cutting%2Dedge,hyperparameters%20for%20machine%20learning%20models.\n",
            "Pausing for 1 minute to comply with crawl limit...\n",
            "Crawling: https://nats.io/\n",
            "Pausing for 1 minute to comply with crawl limit...\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# Crawl websites and handle responses\n",
        "url_response = {}\n",
        "crawl_per_min = 1  # Max crawl per minute\n",
        "\n",
        "# Track crawls\n",
        "crawled_websites = 0\n",
        "scraped_pages = 0\n",
        "\n",
        "for i, website_dict in enumerate(website_list):\n",
        "    url = website_dict.get('URL')\n",
        "    print(f\"Crawling: {url}\")\n",
        "\n",
        "    try:\n",
        "        response = app.crawl_url(\n",
        "            url,\n",
        "            params={\n",
        "                'limit': 5,  # Limit pages to scrape per site.\n",
        "                'scrapeOptions': {'formats': ['markdown', 'html']}\n",
        "            }\n",
        "        )\n",
        "        crawled_websites += 1\n",
        "\n",
        "    except Exception as exc:\n",
        "        print(f\"Failed to fetch {url} -> {exc}\")\n",
        "        continue\n",
        "\n",
        "    # Store the scraped data and associated info in the response dict\n",
        "    url_response[url] = {\n",
        "        \"scraped_data\": response.get(\"data\"),\n",
        "        \"csv_data\": website_dict\n",
        "    }\n",
        "\n",
        "    # Pause to comply with crawl per minute limit for free version its 1 crawl per minute\n",
        "    if i!=len(website_list) and (i + 1) % crawl_per_min == 0:\n",
        "        print(\"Pausing for 1 minute to comply with crawl limit...\")\n",
        "        time.sleep(60)  # Pause for 1 minute after every crawl\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwu6VIMvTKp1"
      },
      "source": [
        "## Create  llamaindex documents from the scraped content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHSEWg7FBdSS"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Document\n",
        "documents = []\n",
        "\n",
        "for _, scraped_content in url_response.items():\n",
        "    csv_data = scraped_content.get(\"csv_data\")\n",
        "    scraped_results = scraped_content.get(\"scraped_data\")\n",
        "\n",
        "    for scraped_site_dict in scraped_results:\n",
        "        for result in scraped_results:\n",
        "            markdown_content = result.get(\"markdown\")\n",
        "            title = result.get(\"metadata\").get(\"title\")\n",
        "            url = result.get(\"metadata\").get(\"sourceURL\")\n",
        "            documents.append(\n",
        "                Document(\n",
        "                    text=markdown_content,\n",
        "                    metadata={\n",
        "                        \"title\": title,\n",
        "                        \"url\": url,\n",
        "                        \"description\": csv_data.get(\"Description\"),\n",
        "                        \"category\": csv_data.get(\"Category\")\n",
        "                    }\n",
        "                )\n",
        "            )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwtRhcpQT294"
      },
      "source": [
        "# Create The RAG Pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDOgwesPI-Kq"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "llm = OpenAI(model=\"gpt-4o-mini\")\n",
        "embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\")\n",
        "text_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8Oi4MiJJQii"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Settings\n",
        "\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model\n",
        "Settings.text_splitter = text_splitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61ZLk2GoJ4VH"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "query_engine = index.as_query_engine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1ImxmsoUNVo"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Markdown, display\n",
        "def display_response(response):\n",
        "    display(Markdown(f\"<b>{response}</b>\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcFcFgCvJ7Y8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "outputId": "d2d90afa-d3bc-4b96-eea7-d87fe7a9fe34"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<b>Bayesian Optimization and Hyperband (BOHB) is an advanced technique for hyperparameter tuning that combines two effective methodologies. \n\nBayesian Optimization is a probabilistic model-based approach that utilizes a surrogate model, typically a Gaussian process, to represent the objective function, such as model accuracy. This method strategically explores the hyperparameter space, making informed decisions about where to sample next, which is particularly advantageous when evaluating the objective function is costly.\n\nHyperband, on the other hand, is a resource allocation strategy that optimally uses a limited budget—whether in terms of time or computational resources—to tune hyperparameters. It focuses on allocating resources to the most promising configurations while discarding those that perform poorly early in the process.\n\nBy integrating these two approaches, BOHB enhances the efficiency of hyperparameter optimization, allowing for effective exploration of large hyperparameter spaces, handling both continuous and categorical parameters. It automates the tuning process, minimizing the need for manual adjustments, and often achieves superior performance compared to traditional methods like grid search and random search.</b>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------\n",
            "Node ID\t 59733826-963c-49e6-befc-9b5ac1c95819\n",
            "Title\t BOHB Hyperparameter Tuning With an Example\n",
            "URL\t https://dzone.com/articles/bayesian-optimization-and-hyperband-bohb-hyperpara#:~:text=Bayesian%20Optimization%20and%20Hyperband%20(BOHB)%20is%20a%20cutting%2Dedge,hyperparameters%20for%20machine%20learning%20models.\n",
            "Score\t 0.7274151454901601\n",
            "Description\t Bayesian Optimization and HyperBand\n",
            "Category\t Hyperparameter Tuning\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Node ID\t 5a4bdf88-e521-42cf-adf5-6b0a02afdedd\n",
            "Title\t BOHB Hyperparameter Tuning With an Example\n",
            "URL\t https://dzone.com/articles/bayesian-optimization-and-hyperband-bohb-hyperpara#:~:text=Bayesian%20Optimization%20and%20Hyperband%20(BOHB)%20is%20a%20cutting%2Dedge,hyperparameters%20for%20machine%20learning%20models.\n",
            "Score\t 0.6926521645672065\n",
            "Description\t Bayesian Optimization and HyperBand\n",
            "Category\t Hyperparameter Tuning\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
          ]
        }
      ],
      "source": [
        "query = \"Explain Bayesian Optimization and Hyperband (BOHB) Hyperparameter Tuning\" # Enter your query here, it should be relevant to the crawled websites\n",
        "res = query_engine.query(query)\n",
        "display_response(res)\n",
        "\n",
        "print(\"-----------------\")\n",
        "# Show the retrieved nodes\n",
        "for src in res.source_nodes:\n",
        "  print(\"Node ID\\t\", src.node_id)\n",
        "  print(\"Title\\t\", src.metadata['title'])\n",
        "  print(\"URL\\t\", src.metadata['url'])\n",
        "  print(\"Score\\t\", src.score)\n",
        "  print(\"Description\\t\", src.metadata.get(\"description\"))\n",
        "  print(\"Category\\t\", src.metadata.get(\"category\"))\n",
        "  print(\"-_\"*20)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Querying not relevant to the crawled websites.\n",
        "\n",
        "query = \"What is qdrant?\"\n",
        "res = query_engine.query(query)\n",
        "display_response(res)\n",
        "\n",
        "print(\"-----------------\")\n",
        "# Show the retrieved nodes\n",
        "for src in res.source_nodes:\n",
        "  print(\"Node ID\\t\", src.node_id)\n",
        "  print(\"Title\\t\", src.metadata['title'])\n",
        "  print(\"URL\\t\", src.metadata['url'])\n",
        "  print(\"Score\\t\", src.score)\n",
        "  print(\"Description\\t\", src.metadata.get(\"description\"))\n",
        "  print(\"Category\\t\", src.metadata.get(\"category\"))\n",
        "  print(\"-_\"*20)"
      ],
      "metadata": {
        "id": "8Uw13m2AVFWf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "2ba31be1-3413-4d59-b2cd-1f914ad3492b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<b>The provided information does not include details about Qdrant. It focuses on a format for the storage of chunked, compressed, N-dimensional arrays. For information about Qdrant, you may need to consult other sources.</b>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------\n",
            "Node ID\t ca003187-fb94-470f-966a-fed50025ab42\n",
            "Title\t Zarr - Zarr\n",
            "URL\t https://zarr.dev/\n",
            "Score\t 0.2227494278283149\n",
            "Description\t Format for the storage of chunked, compressed, N-dimensional arrays\n",
            "Category\t Data Storage\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8zQtPrap1miR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}